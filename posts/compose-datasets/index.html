<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Compose Datasets, Don't Inherit Them | Don't Repeat Yourself</title><meta name=keywords content="cleancode"><meta name=description content="In relatively young disciplines, like deep learning, people tend to leave behind old principles.
Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go.
Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window.
I am no exception in this regard so let me tell you how I &ldquo;re-learned&rdquo; the tried and true design pattern of &ldquo;Composition over Inheritance&rdquo;."><meta name=author content><link rel=canonical href=https://tilman151.github.io/posts/compose-datasets/><link crossorigin=anonymous href=/assets/css/stylesheet.1ae8e05e4717f0de04d72299656239a69424e919bf443379fd60697bbc9bba35.css integrity="sha256-GujgXkcX8N4E1yKZZWI5ppQk6Rm/RDN5/WBpe7ybujU=" rel="preload stylesheet" as=style><link rel=icon href=https://tilman151.github.io/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tilman151.github.io/img/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tilman151.github.io/img/favicon-32x32.png><link rel=apple-touch-icon href=https://tilman151.github.io/img/apple-touch-icon.png><link rel=mask-icon href=https://tilman151.github.io/img/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tilman151.github.io/posts/compose-datasets/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://tilman151.github.io/posts/compose-datasets/"><meta property="og:site_name" content="Don't Repeat Yourself"><meta property="og:title" content="Compose Datasets, Don't Inherit Them"><meta property="og:description" content="In relatively young disciplines, like deep learning, people tend to leave behind old principles. Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go. Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window. I am no exception in this regard so let me tell you how I “re-learned” the tried and true design pattern of “Composition over Inheritance”."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-30T00:00:00+00:00"><meta property="article:modified_time" content="2022-01-30T00:00:00+00:00"><meta property="article:tag" content="Cleancode"><meta property="og:image" content="https://tilman151.github.io/posts/compose-datasets/og.png"><meta property="og:image:width" content="1600"><meta property="og:image:height" content="900"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://tilman151.github.io/posts/compose-datasets/og.png"><meta name=twitter:title content="Compose Datasets, Don't Inherit Them"><meta name=twitter:description content="In relatively young disciplines, like deep learning, people tend to leave behind old principles.
Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go.
Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window.
I am no exception in this regard so let me tell you how I &ldquo;re-learned&rdquo; the tried and true design pattern of &ldquo;Composition over Inheritance&rdquo;."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tilman151.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Compose Datasets, Don't Inherit Them","item":"https://tilman151.github.io/posts/compose-datasets/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Compose Datasets, Don't Inherit Them","name":"Compose Datasets, Don\u0027t Inherit Them","description":"In relatively young disciplines, like deep learning, people tend to leave behind old principles. Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go. Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window. I am no exception in this regard so let me tell you how I \u0026ldquo;re-learned\u0026rdquo; the tried and true design pattern of \u0026ldquo;Composition over Inheritance\u0026rdquo;.\n","keywords":["cleancode"],"articleBody":"In relatively young disciplines, like deep learning, people tend to leave behind old principles. Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go. Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window. I am no exception in this regard so let me tell you how I “re-learned” the tried and true design pattern of “Composition over Inheritance”.\nDataset code is, in my opinion, one of the messiest and hardest parts of any deep learning project. And with datasets, I mean the part of your program that loads data from disk and puts it into tensors for your model to consume during training. Data comes in all shapes and forms. Even if it is intended for the same task, transforming it into a representation suitable for your model is complicated. Furthermore, there are often as many hyperparameters to be configured in your dataset as in your model architecture. So the question here is:\nHow do I write datasets that are reusable, configurable, and handle different kinds of data without repeating myself all the time?\nIn computer vision, this is less of a problem. Most data comes as JPEGs in folders. Just use torchvision.datasets.DatasetFolder, have a nice day, and thank you. In other disciplines, like NLP, it is a little more complicated. You get one or multiple CSVs in several dialects, heaps of plain text files with labels encoded in the file name, databases, or even worse. The things you want to do with this data, in the end, are pretty similar: tokenize it, build a vocabulary, get token IDs for your samples. I will use a simple text classification task as an example to show you how my dataset code tended to evolve, what went wrong, and how I made it better. By the way, we will be using PyTorch and its companion package torchtext.\nYou can find all the code I am going to use at: https://github.com/tilman151/composing-datasets. As I mentioned, we will look at how the code changes over time, so I am going to show code snippets at different points in the commit history of the repository. The relevant commits have a version tag (e.g. v0.1.0) that I will use to refer to them. Each version is tested and should be usable in its current state if you want to try it out.\nHumble Beginnings We want to start small and do some simple, preliminary experiments with our text classification architecture. Therefore, we will use the Automated Hate Speech Detection and the Problem of Offensive Language Dataset or hate speech dataset for short. It consists of tweets that are either labeled hate speech, offensive, or neither. It is rather small, at about 2.5MB, and comes in a single CSV file. We can write a short and simple PyTorch dataset to load it and tag it as v0.1.0.\nfrom torch.utils.data import Dataset # [...] class HateSpeechDataset(Dataset): DOWNLOAD_URL: str = \"...\" DATA_ROOT: str = \"...\" DATA_FILE: str = \"...\" def __init__(self) -\u003e None: os.makedirs(self.DATA_ROOT, exist_ok=True) if not os.path.exists(self.DATA_FILE): _download_data(self.DOWNLOAD_URL, self.DATA_FILE) self.text, self.labels = self._load_data() self.tokenizer = torchtext.data.get_tokenizer(\"basic_english\") self.tokens = [self.tokenizer(text) for text in self.text] self.vocab = torchtext.vocab.build_vocab_from_iterator(self.tokens) self.vocab.set_default_index(len(self.vocab)) self.token_ids = [self._tokens_to_tensor(tokens) for tokens in self.tokens] self.labels = [torch.tensor(label, dtype=torch.long) for label in self.labels] # [...] def __getitem__(self, index: int) -\u003e Tuple[torch.Tensor, torch.Tensor]: return self.token_ids[index], self.labels[index] def __len__(self) -\u003e int: return len(self.text) The dataset creates a folder for the data and downloads the CSV file if it is not already there. As the amount of data is relatively small, we can load it into memory all at once. We use the simple basic_english tokenizer from the torchtext package and build a vocabulary from all tokens and a default out-of-vocabulary token. At last, the token ids for each tweet and the label are stored as tensors in two lists. The __get_item__ function simply indexes these two lists. The advantage of this dataset is that retrieving samples directly from memory is fast but comes at the cost of a longer initialization time.\nInstantiating the dataset is as simple as it gets because there is nothing to configure. But, as I said, data representation matters for deep learning, so we may want to experiment with different ones. Let’s say, we want to try out another tokenizer, e.g. one from the revtok package. Good for us that the get_tokenizer function from torchtext already provides a way to retrieve this tokenizer by passing \"revtok\". We just add a string argument named tokenizer to the constructor and pass it to the function. Making this argument optional preserves the original feature of calling it without arguments to get the basic_english tokenizer. This is version 0.2.0 of our dataset.\nclass HateSpeechDataset(Dataset): DOWNLOAD_URL: str = \"...\" DATA_ROOT: str = \"...\" DATA_FILE: str = \"...\" def __init__(self, tokenizer: Optional[str] = None) -\u003e None: os.makedirs(self.DATA_ROOT, exist_ok=True) if not os.path.exists(self.DATA_FILE): _download_data(self.DOWNLOAD_URL, self.DATA_FILE) self.text, self.labels = self._load_data() self.tokenizer = self._get_tokenizer(tokenizer) self.tokens = [self.tokenizer(text) for text in self.text] self.vocab = torchtext.vocab.build_vocab_from_iterator(self.tokens) self.vocab.set_default_index(len(self.vocab)) self.token_ids = [self._tokens_to_tensor(tokens) for tokens in self.tokens] self.labels = [torch.tensor(label, dtype=torch.long) for label in self.labels] # [...] def _get_tokenizer(self, tokenizer: Optional[str]) -\u003e Callable: if tokenizer is None: tokenizer = torchtext.data.get_tokenizer(\"basic_english\") else: tokenizer = torchtext.data.get_tokenizer(tokenizer) return tokenizer # [...] Through get_tokenizer we have access to several tokenizers, like spacy or moses, which makes it quite flexible. All in all, not a big code change.\nThe More, the Merrier Ok, now we fooled around enough with our tiny hate speech dataset and want to go a little bigger. What about the Large Movie Review Dataset or Imdb dataset for short? We get a user’s movie review and have to predict the number of stars this user gave the movie. Each review comes as a plain text file with the star rating encoded in the file’s name.\nBasically, we want to do exactly the same data processing as with the hate speech dataset but have to load a different data format. Our Intro to CS knowledge tells us that this is a job for inheritance. Put the shared functionality in a base class and do the specialized data reading in a child class. This brings us to version 0.3.0 of our code.\nclass TextClassificationDataset(Dataset, metaclass=ABCMeta): DOWNLOAD_URL: str DATA_ROOT: str DATA_FILE: str # [...] @abstractmethod def _load_data(self) -\u003e Tuple[List[str], List[int]]: pass # [...] class HateSpeechDataset(TextClassificationDataset): DOWNLOAD_URL: str = \"...\" DATA_ROOT: str = \"...\" DATA_FILE: str = \"...\" def _load_data(self) -\u003e Tuple[List[str], List[int]]: # [...] class ImdbDataset(TextClassificationDataset): DOWNLOAD_URL: str = \"...\" DATA_ROOT: str = \"...\" DATA_FILE: str = \"...\" # [...] def __init__(self, split: str, tokenizer: Optional[str] = None) -\u003e None: if split not in [\"train\", \"test\"]: raise ValueError(\"Unknown split supplied. Use either 'train' or 'test'.\") self.split = split super(ImdbDataset, self).__init__(tokenizer) def _load_data(self) -\u003e Tuple[List[str], List[int]]: # [...] Our base class TextClassificationDataset handles everything besides reading the data from disk. Instead, it has an abstract _load_data function that has to be implemented by its children. This makes adding new datasets really easy. Just provide the class variables (DOWNLOAD_URL, etc.) and implement _load_data. For the Imdb dataset, we have to provide a separate constructor, too, because it comes with a pre-defined train-test split. We have to pass the desired split as an argument so that _load_data knows which one to load.\nWith the automatic refactoring tools of modern IDEs, this is, again, not a big change but enables us to reuse our code for new data with ease.\nDiverging Paths We did some experiments on the Imdb data and came to the conclusion that we need more control over the tokenizer. For example, the revtok tokenizer has options that control the capitalization of tokens and if splits should be made on punctuation. We want to try these new configurations on the smaller hate speech dataset but then use it on the Imdb data with minimal effort.\nAn easy solution would be adding a dictionary argument to the dataset constructor which contains the keyword arguments to configure the tokenizer. The code below is version 0.3.1-a of our code and showcases this approach for the revtok tokenizer.\nclass TextClassificationDataset(Dataset, metaclass=ABCMeta): DOWNLOAD_URL: str DATA_ROOT: str DATA_FILE: str def __init__(self, tokenizer: Optional[str] = None, **kwargs) -\u003e None: os.makedirs(self.DATA_ROOT, exist_ok=True) if not os.path.exists(self.DATA_FILE): _download_data(self.DOWNLOAD_URL, self.DATA_ROOT) self.text, self.labels = self._load_data() self.tokenizer = self._get_tokenizer(tokenizer, kwargs) self.tokens = [self.tokenizer(text) for text in self.text] self.vocab = torchtext.vocab.build_vocab_from_iterator(self.tokens) self.vocab.set_default_index(len(self.vocab)) self.token_ids = [self._tokens_to_tensor(tokens) for tokens in self.tokens] self.labels = [torch.tensor(label, dtype=torch.long) for label in self.labels] # [...] def _get_tokenizer( self, tokenizer: Optional[str], kwargs: Dict[Any, Any] ) -\u003e Callable: if tokenizer is None: tokenizer = torchtext.data.get_tokenizer(\"basic_english\") elif not kwargs: tokenizer = torchtext.data.get_tokenizer(tokenizer) elif tokenizer == \"revtok\": tokenizer = _RevtokTokenizer(kwargs) else: raise ValueError(f\"Unknown tokenizer '{tokenizer}'.\") return tokenizer # [...] class _RevtokTokenizer: def __init__(self, kwargs: Dict[Any, Any]) -\u003e None: self.kwargs = kwargs def __call__(self, x: str) -\u003e List[str]: return revtok.tokenize(x, **self.kwargs) We needed to introduce a wrapper class for the revtok tokenizer because it is just a function. A lambda function may have been sufficient but this way it is easier to unit test. The _get_tokenizer function is now a little more complicated as we have to check for revtok and kwargs specifically, and cannot solely rely on the torchtext function anymore. We would need to change this function for each new tokenizer we want to configure via kwargs. Of course, we need to change the constructor of the Imdb dataset, as well, for passing kwargs to the super class’ constructor.\nThis approach is simple but has some drawbacks. First, as mentioned before, you have to change _get_tokenizer for each new tokenizer. Second, you cannot use auto-complete or other IDE functions to look up the arguments for your chosen tokenizer. Third, if we want to add more configurable elements, the number of constructor arguments will get large quickly. Furthermore, the names of these arguments get longer, too, as we would have to specify that kwargs is in fact tokenizer_kwargs. All of this makes this version a bit clunky to use. Could more inheritance solve these problems? We could use specialized classes with fitting constructors for each tokenizer and pass arguments without resorting to dictionaries. Inheritance helped once, why not twice?\nWe can make _get_tokenizer abstract and create a child that gives us the torchtext-based implementation and another one that implements a configurable revtok tokenizer specifically. For both, hate speech and Imdb, we now have two classes. One that descends from the base class plus the torchtext implementation and one descending from the base class plus the revtok implementation. This version is tagged v0.3.1-b.\nEven though this solves the second problem of 0.3.1-a, it creates several others. We introduce even more classes. So many that I even refrain from showing them here. For new data and new tokenizers, we need not only a new class for both but additional classes for each resulting data-tokenizer combination. A combinatorial explosion of classes is imminent. Thoroughly testing all of these combinations is an exercise in futility. Multiple inheritance has its own problems, as well. The inheritance order in Python is from right to left. This means that the class implementing _load_data has to be left of the class defining the abstract _load_data to override it. It won’t work the other way around.\nThis problem of an ever expanding-amount of subclasses is a known problem when using inheritance extensively. It may be fine while inheritance follows only one path of specialization but fails when multiple paths are available. In our example, one path is each of our datasets (hate speech and Imdb) and the second one is the tokenizer to use. If we were to add a third path, like configuring pre-trained embeddings, the number of necessary subclasses would increase even more.\nComposing a Solution Composition is a technique where an object is constructed from other objects in a building block fashion. Each building block fulfills a specific functionality but capsules its inner workings away from the outer object. In our case, we have two different types of building blocks: the ones that read the text and the labels from disk, and the tokenizer. Composing a dataset would be as easy as passing the constructor an object that reads the data of our choice and a tokenizer object. Both are configured outside the dataset object which gives us great flexibility. Version 0.4.0 of our code adheres to these principles.\nclass TextClassificationDataset(Dataset): def __init__( self, dataset: Dataset, tokenizer: Union[str, Callable, None] = None ) -\u003e None: self.dataset = dataset self.text, self.labels = tuple(zip(*self.dataset)) self.tokenizer = self._get_tokenizer(tokenizer) self.tokens = [self.tokenizer(text) for text in self.text] self.vocab = torchtext.vocab.build_vocab_from_iterator(self.tokens) self.vocab.set_default_index(len(self.vocab)) self.token_ids = [self._tokens_to_tensor(tokens) for tokens in self.tokens] self.labels = [torch.tensor(label, dtype=torch.long) for label in self.labels] def _get_tokenizer(self, tokenizer: Optional[str]) -\u003e Callable: if tokenizer is None: tokenizer = torchtext.data.get_tokenizer(\"basic_english\") else: tokenizer = torchtext.data.get_tokenizer(tokenizer) return tokenizer # [...] class HateSpeechDataset(Dataset, DownloadDataMixin): # [...] def __init__(self): self._download_data() self.text, self.labels = self._load_data() # [...] def __getitem__(self, index: int) -\u003e Tuple[str, int]: return self.text[index], self.labels[index] class ImdbDataset(Dataset, DownloadDataMixin): # [...] def __init__(self, split: str) -\u003e None: if split not in [\"train\", \"test\"]: raise ValueError(\"Unknown split supplied. Use either 'train' or 'test'.\") self.split = split self._download_data() self.text, self.labels = self._load_data() # [...] def __getitem__(self, index: int) -\u003e Tuple[str, int]: return self.text[index], self.labels[index] The classes reading the data from disk adhere to the torch dataset interface and return a text string and its integer label. Any object that implements this interface can be passed to our TextClassificationDataset. We can reuse the _get_tokenizer function from version 0.3.0 because of the versatile design of get_tokenizer from torchtext. It can receive a callable object and simply pass it through. This way, we can still use tokenizer as a simple string argument or pass a fully configured tokenizer object. To instantiate a hate speech dataset with a custom revtok tokenizer is as simple as:\nTextClassificationDataset(HateSpeechDataset(), lambda x: revtok.tokenize(x, split_punctuation=True)) Adding new data this way is as easy as implementing the torch dataset interface. Most tokenizers can be used out of the box, as they follow the same interface of just being a callable.\nComposing Even Further We have seen how composing our dataset makes our code more flexible. Extending its functionality is simple, as well, by composing new datasets from TextClassificationDataset. Need a train-val split for the hate speech data?\nfrom torch.utils.data import random_split hate_speech = TextClassificationDataset(HateSpeechDataset()) num_samples = len(hate_speech) train, val = random_split(hate_speech, [num_samples // 2] * 2) The random_split function internally uses the Subset class which takes our base dataset and returns only a specified set of samples. But now the vocabulary still contains tokens that are only in the validation data. We may not want that. No problem at all for our composed dataset.\nhate_speech = HateSpeechDataset() num_samples = len(hate_speech) train, val = random_split(hate_speech, [num_samples // 2] * 2) train = TextClassificationDataset(train) val = TextClassificationDataset(val) This way the data is split before the vocabulary is constructed so that we avoid data leakage. Now, let’s imagine we want to use a pre-trained embedding layer. Do we need to change our dataset class? No, we don’t. We simply compose a new dataset.\nclass PreTrainedEmbeddingDataset(Dataset): def __init__(self, dataset: Dataset, embeddings: torch.nn.Embedding) -\u003e None: self.dataset = dataset self.embeddings = embeddings def __getitem__(self, index: int) -\u003e Tuple[torch.Tensor, torch.Tensor]: token_ids, label = self.dataset(index) embedded_tokens = self.embeddings(token_ids) return embedded_tokens, label def __len__(self) -\u003e int: return len(self.dataset) Because all of our classes follow the same dataset interface, we can just keep stacking them. This keeps the base dataset class short and adds functionality as needed.\nAnother advantage of composing datasets comes with powerful configuration frameworks like hydra. As hydra uses composition to structure config files, it works best with a codebase that uses composition, too. But this is a topic for a later article.\nIn Conclusion This concludes our journey through the commit history of this project. I hope you agree with me that version 0.4.0 is the superior version of our codebase. It is relatively short, it is flexible, and it is easily extended. By the way, it was easiest to test, as well. You can check out the tests folder to see for yourself.\nComposition over inheritance really seems to be not only a phrase but a tried and true principle of software design. Even for deep learning. As always, these design patterns have to be taken with a grain of salt. Is composition better for each problem? Probably not. If your problem is of limited scope, inheritance may be the quicker solution. It really depends. Just keep in mind that composition is in your toolbox so that you don’t repeat yourself.\n","wordCount":"2751","inLanguage":"en","datePublished":"2022-01-30T00:00:00Z","dateModified":"2022-01-30T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tilman151.github.io/posts/compose-datasets/"},"publisher":{"@type":"Organization","name":"Don't Repeat Yourself","logo":{"@type":"ImageObject","url":"https://tilman151.github.io/img/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://tilman151.github.io/ accesskey=h title="Don't Repeat Yourself (Alt + H)">Don't Repeat Yourself</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tilman151.github.io/publications/ title=Publications><span>Publications</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Compose Datasets, Don't Inherit Them</h1><div class=post-meta><span title='2022-01-30 00:00:00 +0000 UTC'>30 January 2022</span>&nbsp;·&nbsp;<span>13 min</span></div></header><div class=post-content><p>In relatively young disciplines, like deep learning, people tend to leave behind old principles.
Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go.
Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window.
I am no exception in this regard so let me tell you how I &ldquo;re-learned&rdquo; the tried and true design pattern of &ldquo;Composition over Inheritance&rdquo;.</p><p>Dataset code is, in my opinion, one of the messiest and hardest parts of any deep learning project.
And with datasets, I mean the part of your program that loads data from disk and puts it into tensors for your model to consume during training.
Data comes in all shapes and forms.
Even if it is intended for the same task, transforming it into a representation suitable for your model is complicated.
Furthermore, there are often as many hyperparameters to be configured in your dataset as in your model architecture.
So the question here is:</p><blockquote><p>How do I write datasets that are reusable, configurable, and handle different kinds of data without repeating myself all the time?</p></blockquote><p>In computer vision, this is less of a problem.
Most data comes as JPEGs in folders.
Just use <code>torchvision.datasets.DatasetFolder</code>, have a nice day, and thank you.
In other disciplines, like NLP, it is a little more complicated.
You get one or multiple CSVs in several dialects, heaps of plain text files with labels encoded in the file name, databases, or even worse.
The things you want to do with this data, in the end, are pretty similar: tokenize it, build a vocabulary, get token IDs for your samples.
I will use a simple text classification task as an example to show you how my dataset code tended to evolve, what went wrong, and how I made it better.
By the way, we will be using PyTorch and its companion package torchtext.</p><p>You can find all the code I am going to use at: <a href=https://github.com/tilman151/composing-datasets>https://github.com/tilman151/composing-datasets</a>.
As I mentioned, we will look at how the code changes over time, so I am going to show code snippets at different points in the commit history of the repository.
The relevant commits have a version tag (e.g. v0.1.0) that I will use to refer to them.
Each version is tested and should be usable in its current state if you want to try it out.</p><h2 id=humble-beginnings>Humble Beginnings<a hidden class=anchor aria-hidden=true href=#humble-beginnings>#</a></h2><p>We want to start small and do some simple, preliminary experiments with our text classification architecture.
Therefore, we will use the <a href=https://github.com/t-davidson/hate-speech-and-offensive-language/>Automated Hate Speech Detection and the Problem of Offensive Language Dataset</a> or hate speech dataset for short.
It consists of tweets that are either labeled hate speech, offensive, or neither.
It is rather small, at about 2.5MB, and comes in a single CSV file.
We can write a short and simple PyTorch dataset to load it and tag it as <a href=https://github.com/tilman151/composing-datasets/tree/v0.1.0><em>v0.1.0</em></a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> Dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HateSpeechDataset</span>(Dataset):
</span></span><span style=display:flex><span>    DOWNLOAD_URL: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_ROOT: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_FILE: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>makedirs(self<span style=color:#f92672>.</span>DATA_ROOT, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(self<span style=color:#f92672>.</span>DATA_FILE):
</span></span><span style=display:flex><span>            _download_data(self<span style=color:#f92672>.</span>DOWNLOAD_URL, self<span style=color:#f92672>.</span>DATA_FILE)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text, self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(<span style=color:#e6db74>&#34;basic_english&#34;</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokens <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>tokenizer(text) <span style=color:#66d9ef>for</span> text <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>text]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>build_vocab_from_iterator(self<span style=color:#f92672>.</span>tokens)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>set_default_index(len(self<span style=color:#f92672>.</span>vocab))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>token_ids <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>_tokens_to_tensor(tokens) <span style=color:#66d9ef>for</span> tokens <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>tokens]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>tensor(label, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long) <span style=color:#66d9ef>for</span> label <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>labels]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>   <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__getitem__</span>(self, index: int) <span style=color:#f92672>-&gt;</span> Tuple[torch<span style=color:#f92672>.</span>Tensor, torch<span style=color:#f92672>.</span>Tensor]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>token_ids[index], self<span style=color:#f92672>.</span>labels[index]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__len__</span>(self) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>text)
</span></span></code></pre></div><p>The dataset creates a folder for the data and downloads the CSV file if it is not already there.
As the amount of data is relatively small, we can load it into memory all at once.
We use the simple <code>basic_english</code> tokenizer from the <code>torchtext</code> package and build a vocabulary from all tokens and a default out-of-vocabulary token.
At last, the token ids for each tweet and the label are stored as tensors in two lists.
The <code>__get_item__</code> function simply indexes these two lists.
The advantage of this dataset is that retrieving samples directly from memory is fast but comes at the cost of a longer initialization time.</p><p>Instantiating the dataset is as simple as it gets because there is nothing to configure.
But, as I said, data representation matters for deep learning, so we may want to experiment with different ones.
Let&rsquo;s say, we want to try out another tokenizer, e.g. one from the revtok package.
Good for us that the <code>get_tokenizer</code> function from <code>torchtext</code> already provides a way to retrieve this tokenizer by passing <code>"revtok"</code>.
We just add a string argument named <code>tokenizer</code> to the constructor and pass it to the function.
Making this argument optional preserves the original feature of calling it without arguments to get the <code>basic_english</code> tokenizer.
This is version <a href=https://github.com/tilman151/composing-datasets/tree/v0.2.0><code>0.2.0</code></a> of our dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HateSpeechDataset</span>(Dataset):
</span></span><span style=display:flex><span>    DOWNLOAD_URL: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_ROOT: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_FILE: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, tokenizer: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>makedirs(self<span style=color:#f92672>.</span>DATA_ROOT, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(self<span style=color:#f92672>.</span>DATA_FILE):
</span></span><span style=display:flex><span>            _download_data(self<span style=color:#f92672>.</span>DOWNLOAD_URL, self<span style=color:#f92672>.</span>DATA_FILE)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text, self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_get_tokenizer(tokenizer)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokens <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>tokenizer(text) <span style=color:#66d9ef>for</span> text <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>text]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>build_vocab_from_iterator(self<span style=color:#f92672>.</span>tokens)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>set_default_index(len(self<span style=color:#f92672>.</span>vocab))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>token_ids <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>_tokens_to_tensor(tokens) <span style=color:#66d9ef>for</span> tokens <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>tokens]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>tensor(label, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long) <span style=color:#66d9ef>for</span> label <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>labels]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_get_tokenizer</span>(self, tokenizer: Optional[str]) <span style=color:#f92672>-&gt;</span> Callable:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> tokenizer <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(<span style=color:#e6db74>&#34;basic_english&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(tokenizer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> tokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span></code></pre></div><p>Through <code>get_tokenizer</code> we have access to several tokenizers, like <code>spacy</code> or <code>moses</code>, which makes it quite flexible.
All in all, not a big code change.</p><h2 id=the-more-the-merrier>The More, the Merrier<a hidden class=anchor aria-hidden=true href=#the-more-the-merrier>#</a></h2><p>Ok, now we fooled around enough with our tiny hate speech dataset and want to go a little bigger.
What about the <a href=https://ai.stanford.edu/~amaas/data/sentiment/>Large Movie Review Dataset</a> or Imdb dataset for short?
We get a user&rsquo;s movie review and have to predict the number of stars this user gave the movie.
Each review comes as a plain text file with the star rating encoded in the file&rsquo;s name.</p><p>Basically, we want to do exactly the same data processing as with the hate speech dataset but have to load a different data format.
Our <em>Intro to CS</em> knowledge tells us that this is a job for inheritance.
Put the shared functionality in a base class and do the specialized data reading in a child class.
This brings us to version <a href=https://github.com/tilman151/composing-datasets/tree/v0.3.0>0.3.0</a> of our code.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TextClassificationDataset</span>(Dataset, metaclass<span style=color:#f92672>=</span>ABCMeta):
</span></span><span style=display:flex><span>    DOWNLOAD_URL: str
</span></span><span style=display:flex><span>    DATA_ROOT: str
</span></span><span style=display:flex><span>    DATA_FILE: str
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_load_data</span>(self) <span style=color:#f92672>-&gt;</span> Tuple[List[str], List[int]]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HateSpeechDataset</span>(TextClassificationDataset):
</span></span><span style=display:flex><span>    DOWNLOAD_URL: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_ROOT: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_FILE: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_load_data</span>(self) <span style=color:#f92672>-&gt;</span> Tuple[List[str], List[int]]:
</span></span><span style=display:flex><span>        <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ImdbDataset</span>(TextClassificationDataset):
</span></span><span style=display:flex><span>    DOWNLOAD_URL: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_ROOT: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>    DATA_FILE: str <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;...&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, split: str, tokenizer: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> split <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;train&#34;</span>, <span style=color:#e6db74>&#34;test&#34;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Unknown split supplied. Use either &#39;train&#39; or &#39;test&#39;.&#34;</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>split <span style=color:#f92672>=</span> split
</span></span><span style=display:flex><span>        super(ImdbDataset, self)<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>(tokenizer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_load_data</span>(self) <span style=color:#f92672>-&gt;</span> Tuple[List[str], List[int]]:
</span></span><span style=display:flex><span>        <span style=color:#75715e># [...]</span>
</span></span></code></pre></div><p>Our base class <code>TextClassificationDataset</code> handles everything besides reading the data from disk.
Instead, it has an abstract <code>_load_data</code> function that has to be implemented by its children.
This makes adding new datasets really easy.
Just provide the class variables (<code>DOWNLOAD_URL</code>, etc.) and implement <code>_load_data</code>.
For the Imdb dataset, we have to provide a separate constructor, too, because it comes with a pre-defined train-test split.
We have to pass the desired split as an argument so that <code>_load_data</code> knows which one to load.</p><p>With the automatic refactoring tools of modern IDEs, this is, again, not a big change but enables us to reuse our code for new data with ease.</p><h2 id=diverging-paths>Diverging Paths<a hidden class=anchor aria-hidden=true href=#diverging-paths>#</a></h2><p>We did some experiments on the Imdb data and came to the conclusion that we need more control over the tokenizer.
For example, the <code>revtok</code> tokenizer has options that control the capitalization of tokens and if splits should be made on punctuation.
We want to try these new configurations on the smaller hate speech dataset but then use it on the Imdb data with minimal effort.</p><p>An easy solution would be adding a dictionary argument to the dataset constructor which contains the keyword arguments to configure the tokenizer.
The code below is version <a href=https://github.com/tilman151/composing-datasets/tree/v0.3.1-a>0.3.1-a</a> of our code and showcases this approach for the <code>revtok</code> tokenizer.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TextClassificationDataset</span>(Dataset, metaclass<span style=color:#f92672>=</span>ABCMeta):
</span></span><span style=display:flex><span>    DOWNLOAD_URL: str
</span></span><span style=display:flex><span>    DATA_ROOT: str
</span></span><span style=display:flex><span>    DATA_FILE: str
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, tokenizer: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>, <span style=color:#f92672>**</span>kwargs) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        os<span style=color:#f92672>.</span>makedirs(self<span style=color:#f92672>.</span>DATA_ROOT, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>exists(self<span style=color:#f92672>.</span>DATA_FILE):
</span></span><span style=display:flex><span>            _download_data(self<span style=color:#f92672>.</span>DOWNLOAD_URL, self<span style=color:#f92672>.</span>DATA_ROOT)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text, self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_get_tokenizer(tokenizer, kwargs)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokens <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>tokenizer(text) <span style=color:#66d9ef>for</span> text <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>text]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>build_vocab_from_iterator(self<span style=color:#f92672>.</span>tokens)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>set_default_index(len(self<span style=color:#f92672>.</span>vocab))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>token_ids <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>_tokens_to_tensor(tokens) <span style=color:#66d9ef>for</span> tokens <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>tokens]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>tensor(label, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long) <span style=color:#66d9ef>for</span> label <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>labels]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_get_tokenizer</span>(
</span></span><span style=display:flex><span>        self, tokenizer: Optional[str], kwargs: Dict[Any, Any]
</span></span><span style=display:flex><span>    ) <span style=color:#f92672>-&gt;</span> Callable:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> tokenizer <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(<span style=color:#e6db74>&#34;basic_english&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> <span style=color:#f92672>not</span> kwargs:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(tokenizer)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> tokenizer <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;revtok&#34;</span>:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> _RevtokTokenizer(kwargs)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Unknown tokenizer &#39;</span><span style=color:#e6db74>{</span>tokenizer<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> tokenizer
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>_RevtokTokenizer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, kwargs: Dict[Any, Any]) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>kwargs <span style=color:#f92672>=</span> kwargs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, x: str) <span style=color:#f92672>-&gt;</span> List[str]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> revtok<span style=color:#f92672>.</span>tokenize(x, <span style=color:#f92672>**</span>self<span style=color:#f92672>.</span>kwargs)
</span></span></code></pre></div><p>We needed to introduce a wrapper class for the <code>revtok</code> tokenizer because it is just a function.
A lambda function may have been sufficient but this way it is easier to unit test.
The <code>_get_tokenizer</code> function is now a little more complicated as we have to check for <code>revtok</code> and <code>kwargs</code> specifically, and cannot solely rely on the <code>torchtext</code> function anymore.
We would need to change this function for each new tokenizer we want to configure via <code>kwargs</code>.
Of course, we need to change the constructor of the Imdb dataset, as well, for passing <code>kwargs</code> to the super class&rsquo; constructor.</p><p>This approach is simple but has some drawbacks.
First, as mentioned before, you have to change <code>_get_tokenizer</code> for each new tokenizer.
Second, you cannot use auto-complete or other IDE functions to look up the arguments for your chosen tokenizer.
Third, if we want to add more configurable elements, the number of constructor arguments will get large quickly.
Furthermore, the names of these arguments get longer, too, as we would have to specify that <code>kwargs</code> is in fact <code>tokenizer_kwargs</code>.
All of this makes this version a bit clunky to use.
Could more inheritance solve these problems?
We could use specialized classes with fitting constructors for each tokenizer and pass arguments without resorting to dictionaries.
Inheritance helped once, why not twice?</p><p>We can make <code>_get_tokenizer</code> abstract and create a child that gives us the <code>torchtext</code>-based implementation and another one that implements a configurable <code>revtok</code> tokenizer specifically.
For both, hate speech and Imdb, we now have two classes.
One that descends from the base class plus the <code>torchtext</code> implementation and one descending from the base class plus the <code>revtok</code> implementation.
This version is tagged <a href=https://github.com/tilman151/composing-datasets/tree/v0.3.1-b>v0.3.1-b</a>.</p><p>Even though this solves the second problem of 0.3.1-a, it creates several others.
We introduce even more classes.
So many that I even refrain from showing them here.
For new data and new tokenizers, we need not only a new class for both but additional classes for each resulting data-tokenizer combination.
A combinatorial explosion of classes is imminent.
Thoroughly testing all of these combinations is an exercise in futility.
Multiple inheritance has its own problems, as well.
The inheritance order in Python is from right to left.
This means that the class implementing <code>_load_data</code> has to be left of the class defining the abstract <code>_load_data</code> to override it.
It won&rsquo;t work the other way around.</p><p>This problem of an ever expanding-amount of subclasses is a known problem when using inheritance extensively.
It may be fine while inheritance follows only one path of specialization but fails when multiple paths are available.
In our example, one path is each of our datasets (hate speech and Imdb) and the second one is the tokenizer to use.
If we were to add a third path, like configuring pre-trained embeddings, the number of necessary subclasses would increase even more.</p><h2 id=composing-a-solution>Composing a Solution<a hidden class=anchor aria-hidden=true href=#composing-a-solution>#</a></h2><p>Composition is a technique where an object is constructed from other objects in a building block fashion.
Each building block fulfills a specific functionality but capsules its inner workings away from the outer object.
In our case, we have two different types of building blocks: the ones that read the text and the labels from disk, and the tokenizer.
Composing a dataset would be as easy as passing the constructor an object that reads the data of our choice and a tokenizer object.
Both are configured outside the dataset object which gives us great flexibility.
Version <a href=https://github.com/tilman151/composing-datasets/tree/v0.4.0>0.4.0</a> of our code adheres to these principles.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TextClassificationDataset</span>(Dataset):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>        self, dataset: Dataset, tokenizer: Union[str, Callable, <span style=color:#66d9ef>None</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    ) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dataset <span style=color:#f92672>=</span> dataset
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text, self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> tuple(zip(<span style=color:#f92672>*</span>self<span style=color:#f92672>.</span>dataset))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_get_tokenizer(tokenizer)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokens <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>tokenizer(text) <span style=color:#66d9ef>for</span> text <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>text]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>build_vocab_from_iterator(self<span style=color:#f92672>.</span>tokens)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>vocab<span style=color:#f92672>.</span>set_default_index(len(self<span style=color:#f92672>.</span>vocab))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>token_ids <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>_tokens_to_tensor(tokens) <span style=color:#66d9ef>for</span> tokens <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>tokens]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> [torch<span style=color:#f92672>.</span>tensor(label, dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>long) <span style=color:#66d9ef>for</span> label <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>labels]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_get_tokenizer</span>(self, tokenizer: Optional[str]) <span style=color:#f92672>-&gt;</span> Callable:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> tokenizer <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(<span style=color:#e6db74>&#34;basic_english&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            tokenizer <span style=color:#f92672>=</span> torchtext<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>get_tokenizer(tokenizer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> tokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HateSpeechDataset</span>(Dataset, DownloadDataMixin):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_download_data()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text, self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__getitem__</span>(self, index: int) <span style=color:#f92672>-&gt;</span> Tuple[str, int]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>text[index], self<span style=color:#f92672>.</span>labels[index]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ImdbDataset</span>(Dataset, DownloadDataMixin):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, split: str) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> split <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;train&#34;</span>, <span style=color:#e6db74>&#34;test&#34;</span>]:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Unknown split supplied. Use either &#39;train&#39; or &#39;test&#39;.&#34;</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>split <span style=color:#f92672>=</span> split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>_download_data()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>text, self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># [...]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__getitem__</span>(self, index: int) <span style=color:#f92672>-&gt;</span> Tuple[str, int]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>text[index], self<span style=color:#f92672>.</span>labels[index]
</span></span></code></pre></div><p>The classes reading the data from disk adhere to the <code>torch</code> dataset interface and return a text string and its integer label.
Any object that implements this interface can be passed to our <code>TextClassificationDataset</code>.
We can reuse the <code>_get_tokenizer</code> function from version 0.3.0 because of the versatile design of <code>get_tokenizer</code> from <code>torchtext</code>.
It can receive a callable object and simply pass it through.
This way, we can still use <code>tokenizer</code> as a simple string argument or pass a fully configured tokenizer object.
To instantiate a hate speech dataset with a custom revtok tokenizer is as simple as:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>TextClassificationDataset(HateSpeechDataset(),
</span></span><span style=display:flex><span>                          <span style=color:#66d9ef>lambda</span> x: revtok<span style=color:#f92672>.</span>tokenize(x, split_punctuation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))
</span></span></code></pre></div><p>Adding new data this way is as easy as implementing the <code>torch</code> dataset interface.
Most tokenizers can be used out of the box, as they follow the same interface of just being a callable.</p><h2 id=composing-even-further>Composing Even Further<a hidden class=anchor aria-hidden=true href=#composing-even-further>#</a></h2><p>We have seen how composing our dataset makes our code more flexible.
Extending its functionality is simple, as well, by composing new datasets from <code>TextClassificationDataset</code>.
Need a train-val split for the hate speech data?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> random_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>hate_speech <span style=color:#f92672>=</span> TextClassificationDataset(HateSpeechDataset())
</span></span><span style=display:flex><span>num_samples <span style=color:#f92672>=</span> len(hate_speech)
</span></span><span style=display:flex><span>train, val <span style=color:#f92672>=</span> random_split(hate_speech, [num_samples <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>] <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><p>The <code>random_split</code> function internally uses the <code>Subset</code> class which takes our base dataset and returns only a specified set of samples.
But now the vocabulary still contains tokens that are only in the validation data.
We may not want that.
No problem at all for our composed dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>hate_speech <span style=color:#f92672>=</span> HateSpeechDataset()
</span></span><span style=display:flex><span>num_samples <span style=color:#f92672>=</span> len(hate_speech)
</span></span><span style=display:flex><span>train, val <span style=color:#f92672>=</span> random_split(hate_speech, [num_samples <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>] <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>train <span style=color:#f92672>=</span> TextClassificationDataset(train)
</span></span><span style=display:flex><span>val <span style=color:#f92672>=</span> TextClassificationDataset(val)
</span></span></code></pre></div><p>This way the data is split before the vocabulary is constructed so that we avoid data leakage.
Now, let&rsquo;s imagine we want to use a pre-trained embedding layer.
Do we need to change our dataset class?
No, we don&rsquo;t.
We simply compose a new dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PreTrainedEmbeddingDataset</span>(Dataset):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, dataset: Dataset, embeddings: torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Embedding) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dataset <span style=color:#f92672>=</span> dataset
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embeddings <span style=color:#f92672>=</span> embeddings
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__getitem__</span>(self, index: int) <span style=color:#f92672>-&gt;</span> Tuple[torch<span style=color:#f92672>.</span>Tensor, torch<span style=color:#f92672>.</span>Tensor]:
</span></span><span style=display:flex><span>        token_ids, label <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>dataset(index)
</span></span><span style=display:flex><span>        embedded_tokens <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embeddings(token_ids)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> embedded_tokens, label
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__len__</span>(self) <span style=color:#f92672>-&gt;</span> int:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>dataset)
</span></span></code></pre></div><p>Because all of our classes follow the same dataset interface, we can just keep stacking them.
This keeps the base dataset class short and adds functionality as needed.</p><p>Another advantage of composing datasets comes with powerful configuration frameworks like <a href=https://hydra.cc/>hydra</a>.
As hydra uses composition to structure config files, it works best with a codebase that uses composition, too.
But this is a topic for a later article.</p><h2 id=in-conclusion>In Conclusion<a hidden class=anchor aria-hidden=true href=#in-conclusion>#</a></h2><p>This concludes our journey through the commit history of this project.
I hope you agree with me that version 0.4.0 is the superior version of our codebase.
It is relatively short, it is flexible, and it is easily extended.
By the way, it was easiest to test, as well.
You can check out the <code>tests</code> folder to see for yourself.</p><p>Composition over inheritance really seems to be not only a phrase but a tried and true principle of software design.
Even for deep learning.
As always, these design patterns have to be taken with a grain of salt.
Is composition better for each problem?
Probably not.
If your problem is of limited scope, inheritance may be the quicker solution.
It really depends.
Just keep in mind that composition is in your toolbox so that you don&rsquo;t repeat yourself.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tilman151.github.io/tags/cleancode/>Cleancode</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://tilman151.github.io/>Don't Repeat Yourself</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg></a><style>#cookie-notice{position:fixed;bottom:20px;left:50%;transform:translateX(-50%);width:90%;max-width:600px;padding:1rem;display:none;background:var(--entry);color:var(--primary);border-radius:var(--radius);box-shadow:0 4px 12px rgba(0,0,0,.2);z-index:1000;text-align:center;align-items:center;justify-content:space-between;gap:1rem}#cookie-notice span{font-size:.9rem;line-height:1.4}.cookie-buttons{display:flex;gap:.5rem;flex-shrink:0}#cookie-notice a{padding:.4rem .8rem;border-radius:var(--radius);background:var(--tertiary);color:var(--primary);font-size:.85rem;cursor:pointer;text-decoration:none}#cookie-notice a:hover{background:var(--secondary);color:var(--theme)}@media(min-width:600px){#cookie-notice{display:none;flex-direction:row;text-align:left}}@media(max-width:599px){#cookie-notice{flex-direction:column;bottom:10px}}</style><div id=cookie-notice><span>I am using third-party cookies to count readers. If you're fine with this, click OK.</span><div class=cookie-buttons><a id=cookie-notice-accept>OK</a>
<a href=/privacy>More info</a></div></div><script>(function(){const e="cookie-notice-dismissed";function t(e){const t=e+"=",n=document.cookie.split(";");for(let e=0;e<n.length;e++){const s=n[e].trim();if(s.indexOf(t)===0)return s.substring(t.length,s.length)}return null}if(t(e)==="true"){const e=document.createElement("script");e.async=!0,e.dataset.id="101271854",e.src="//static.getclicky.com/js",document.head.appendChild(e)}else{const t=document.getElementById("cookie-notice");t.style.display="flex",document.getElementById("cookie-notice-accept").addEventListener("click",function(){const n=new Date;n.setTime(n.getTime()+31*24*60*60*1e3),document.cookie=e+"=true; expires="+n.toUTCString()+"; path=/; SameSite=Lax",t.style.display="none",location.reload()})}})()</script><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>