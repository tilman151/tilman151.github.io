<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Testing FastHTML Dashboards | Don't Repeat Yourself</title>
<meta name=keywords content="fasthtml,testing,cleancode"><meta name=description content="Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.).
Often, these dashboards are hacked together in record time to meet a presentation deadline.
Now imagine this: you built a dashboard for showcasing your latest model to your team.
Instead of your go-to tool, Streamlit, you decided to try out FastHTML, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger.
Your team lead is so impressed with your model that they want to show it to the whole company.
That is your chance to shine!
With FastHTML, you don&rsquo;t have to worry about scaling to a bigger audience.
But wait: are you sure your dashboard is really working as expected?
How can you be certain nothing fails if the CEO happens to use it?
Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing."><meta name=author content><link rel=canonical href=https://tilman151.github.io/posts/testing-fasthtml/><link crossorigin=anonymous href=/assets/css/stylesheet.5cb2fc31fd4c267a86f6f77cd180c44dd566ab953b1f23bd24d4ed27f19f73b4.css integrity="sha256-XLL8Mf1MJnqG9vd80YDETdVmq5U7HyO9JNTtJ/Gfc7Q=" rel="preload stylesheet" as=style><link rel=icon href=https://tilman151.github.io/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tilman151.github.io/img/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tilman151.github.io/img/favicon-32x32.png><link rel=apple-touch-icon href=https://tilman151.github.io/img/apple-touch-icon.png><link rel=mask-icon href=https://tilman151.github.io/img/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tilman151.github.io/posts/testing-fasthtml/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Testing FastHTML Dashboards"><meta property="og:description" content="Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.).
Often, these dashboards are hacked together in record time to meet a presentation deadline.
Now imagine this: you built a dashboard for showcasing your latest model to your team.
Instead of your go-to tool, Streamlit, you decided to try out FastHTML, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger.
Your team lead is so impressed with your model that they want to show it to the whole company.
That is your chance to shine!
With FastHTML, you don&rsquo;t have to worry about scaling to a bigger audience.
But wait: are you sure your dashboard is really working as expected?
How can you be certain nothing fails if the CEO happens to use it?
Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing."><meta property="og:type" content="article"><meta property="og:url" content="https://tilman151.github.io/posts/testing-fasthtml/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-15T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Testing FastHTML Dashboards"><meta name=twitter:description content="Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.).
Often, these dashboards are hacked together in record time to meet a presentation deadline.
Now imagine this: you built a dashboard for showcasing your latest model to your team.
Instead of your go-to tool, Streamlit, you decided to try out FastHTML, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger.
Your team lead is so impressed with your model that they want to show it to the whole company.
That is your chance to shine!
With FastHTML, you don&rsquo;t have to worry about scaling to a bigger audience.
But wait: are you sure your dashboard is really working as expected?
How can you be certain nothing fails if the CEO happens to use it?
Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tilman151.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Testing FastHTML Dashboards","item":"https://tilman151.github.io/posts/testing-fasthtml/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Testing FastHTML Dashboards","name":"Testing FastHTML Dashboards","description":"Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.). Often, these dashboards are hacked together in record time to meet a presentation deadline. Now imagine this: you built a dashboard for showcasing your latest model to your team. Instead of your go-to tool, Streamlit, you decided to try out FastHTML, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger. Your team lead is so impressed with your model that they want to show it to the whole company. That is your chance to shine! With FastHTML, you don\u0026rsquo;t have to worry about scaling to a bigger audience. But wait: are you sure your dashboard is really working as expected? How can you be certain nothing fails if the CEO happens to use it? Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing.\n","keywords":["fasthtml","testing","cleancode"],"articleBody":"Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.). Often, these dashboards are hacked together in record time to meet a presentation deadline. Now imagine this: you built a dashboard for showcasing your latest model to your team. Instead of your go-to tool, Streamlit, you decided to try out FastHTML, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger. Your team lead is so impressed with your model that they want to show it to the whole company. That is your chance to shine! With FastHTML, you don’t have to worry about scaling to a bigger audience. But wait: are you sure your dashboard is really working as expected? How can you be certain nothing fails if the CEO happens to use it? Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing.\nThank goodness you found this post! As I was in a similar situation and came up empty-handed, I needed to figure it out myself. This post is intended for data people who want to test their FastHTML dashboards. If you’re a web developer, you might find this post too basic, but feel free to give feedback on how to improve the testing process described here. But first, we need to go over some basics of automated testing.\nTypes of Automated Tests There are several types of automated tests and even more definitions to each. We will focus on three types here: unit tests, integration tests, and end-to-end tests. There are many a StackOverflow post discussing where to draw the line between these types. I was always of the opinion that one should be pragmatic about it, as long as the following is true:\nUnit tests are extremely fast, integration tests are reasonably fast, and end-to-end tests don’t take forever to run.\nEach test of any type needs to be independent of the others, so you can run them in any order. Furthermore, your test suite should have a lot of unit tests, fewer integration tests, and even fewer end-to-end tests. We will explore why in the following sections.\nUnit Tests The term unit test is often synonymous with automatic test. To be precise, unit tests check the functionality of the smallest code part, usually a function or a method. They need next to no setup, don’t interact with code that is not under test, and run lightning-fast. Try to check all edge cases, so you don’t have to do it with the other test types. If you develop test-driven, these tests would be written in advance to define the functionality of your code. But, as only a few people have the discipline to do so, unit tests are often written after the fact, too. Because they’re so fast, you can, in theory, run them every time you save your code. Unit tests are the most numerous in your suite, because the smallest code parts are the most numerous in your codebase.\nIntegration Tests Integrations tests check if the parts of your code integrate well with each other. In our case, this could be the interaction between the dashboard endpoints and the database code. They need some setup to ensure that the parts under test are connected as they would be in production. Integration tests are a little slower than unit tests, but still fast enough to run them when you push to remote. They’re fewer than unit tests, because there should be fewer combinations in your codebase than individual code parts. How to make sure this is the case is a topic for another post.\nEnd-to-End Tests End-to-end tests check if a user interaction can be successfully executed. They’re often mapped to user stories like “As a user, I want to log in to the dashboard and expect to see the start page afterward.” These tests are the slowest of the bunch, because they need to set up the whole application before running. End-to-end tests are the fewest in your suite, because they’re the most complex to write and maintain. They’re also the most brittle, tending to break over minor changes like moving or renaming a button.\nOur Example Dashboard We will explore our test strategy with a simple example dashboard. It shows a text input field and a button labeled “Ask.” When the user enters a question in the input field and clicks the button, the dashboard disables the input field and displays the LLM-generated answer below with a button to reset the dashboard. The dashboard also features a button that displays the last ten questions asked which are stored in a Sqlite database. This is as simple as it gets, but still provides ample opportunity for testing. And of course, we make our lives more complicated than necessary by including LLM functionality. It is 2024, after all.\nWe use uv for managing dependencies, SQLAlchemy for interacting with the database, pytest as our testing framework, and Poe the Poet as our task runner. For instructions on how to set up the project, refer to the README.\nUnit Tests for the Database First, we want to test the code that reads and writes the questions from and to the database. I already hear the first people scoffing, “But that’s not a unit test, that’s an integration test!” As I said, let us be pragmatic about it. Testing database interaction code without a database is nearly useless. Additionally, we’re using Sqlite, which is an embedded database and part of the Python standard library. You would not consider testing code that uses NumPy an integration test because it includes a C library, would you?\nWith that out of the way, there is some setup we need to do. We need an empty database in working order before each test. Remember, automated tests need to be independent, so the remains of the last test need to be removed from the database. We use the following fixtures for this:\n@pytest.fixture def clean_database(tmp_database): yield tmp_database engine = sa.create_engine(tmp_database) with engine.connect() as connection: database.metadata.drop_all(connection) connection.commit() _init_database(engine) engine.dispose() @pytest.fixture(scope=\"module\") def tmp_database(tmp_database_path): engine = sa.create_engine(tmp_database_path) _init_database(engine) engine.dispose() return tmp_database_path @pytest.fixture(scope=\"module\") def tmp_database_path(tmp_path_factory, request): path = tmp_path_factory.mktemp(request.module.__name__ + \"_data\", numbered=True) path = path / \"inventory.db\" return f\"sqlite:///{path}\" Let us start from the bottom:\nThe tmp_database_path fixture creates a connection string for our database, pointing to a temporary directory. The tmp_database fixture takes the connection string, creates the database, and initializes it with the _init_database function, which is omitted for brevity. The clean_database fixture takes the connection string to the initialized database, yields it to the test function and cleans up afterward by reinitializing the database. But why so complicated? Why not do it in one fixture? The answer is saving time. If we create a new database for each test, we would waste some time, slowing down our unit tests. Remember, they need to be extremely fast. By scoping tmp_database_path and tmp_database to module level, pytest runs these fixtures only once per test module. This way, the database is only set up once and clean_database merely reinitializes it.\nThe last piece of setup concerns the way we connect to the database itself. The database module of the dashboard maintains a global instance of an SQLAlchemy engine that controls the database connections. This way, we can spin up connections to the database quickly without needing to establish the engine first. Unfortunately, this also introduces a global state into our app. Global state is always complicated for automated testing, as it breaks test independence. The following fixture solves this problem for us:\n@pytest.fixture def database_module(): from app import database importlib.reload(database) return database This fixture supplies a freshly imported database module for each of our tests, effectively resetting the global state.\nNow for the unit tests themselves. We will exemplify the process on the append_to_history function. Here is the test code:\ndef test_append_to_history(database_module, clean_database): database_module.connect(clean_database) database_module.append_to_history(\"question0\", \"answer0\") database_module.append_to_history(\"question1\", \"answer1\") with database_module._get_connection() as conn: result = conn.execute(database_module.history.select()).fetchall() assert len(result) == 2 for i, row in enumerate(result): assert row.question == f\"question{i}\" assert row.answer == f\"answer{i}\" assert isinstance(row.created_at, datetime.datetime) assert result[0].created_at \u003c= result[1].created_at The function under test takes a question and the corresponding answer and inserts it into the history table. The database is expected to supply the current timestamp for the created_at column. With our test function, we first connect to the database, insert two lines of data, and retrieve them again. We check if the table really contains only these two entries and if the created_at column was filled correctly. Two fundamental unit testing practices can be observed here:\nFail for the right reason: unit tests are not only intended to check correctness, but also to hint in the direction of a bug. Whether there is only one row in the table, the rows are swapped or the created_at column is unfilled, each broken assumption fails on the appropriate assert. This makes debugging much easier. Test only one unit of code: even though, there is a function to retrieve questions from the database in our module, we don’t use it here. If we use it, we would introduce another piece of code under test, making this an integration test. The test_database module contains more tests than this one. You can find the complete module here. The tests can be run in the activated virtual environment with the following command:\npoe unit Integration Tests for the Routes UPDATE: in a previous version of this article, the integration tests made use of regexes. This was brittle and did a poor job of conveying the test’s intent. Thanks to Jeremy Howard’s feedback, the tests now use the much more suitable XPath functionality of the lxml package.\nNext, we will test if sending requests to the routes of our dashboard works as expected. We will use the starlette.testclient.TestClient for this. It allows us to simulate HTTP requests to our dashboard without spinning up a server. The following fixture sets up the test client:\n@pytest.fixture def client(clean_database, monkeypatch): monkeypatch.setenv(\"APP_DATABASE_URL\", clean_database) from app.main import app client = TestClient(app) client.headers[\"HX-Request\"] = \"true\" database.connect(clean_database) yield client database.disconnect() The monkeypatch fixture is used to temporarily set the database URL environment variable. This way, we can connect the client to the database we set up in the clean_database fixture. As you can see, we’re using our database.connect and database.disconnect functions. This is possible because integration tests are only executed once all unit tests have passed. We can, therefore, assume that these functions are working as intended. This demonstrates the importance of the test hierarchy, as we can now use our high-level functions in the integration tests, making them much more concise. The client fixture also sets the HX-Request header to true. This tells FastHTML that the request is coming from HTMX and the result should be an HTML fragment. Otherwise, FastHTML would wrap the result in additional HTML tags, which would complicate the checks.\nLet us first look at a test for the /ask route. It expects a POST request with a JSON body containing a question. The returned fragment is intended to be swapped with the question input field and submit button.\ndef test_ask_mocked_answer(client, mocker): mock_generate = mocker.patch(\"ollama.generate\") mock_generate.return_value = {\"response\": \"answer0\"} response = client.post(\"/ask\", data={\"question\": \"question0\"}) assert response.status_code == 200 html = lxml.html.fromstring(response.text) assert html.xpath(\"//input[@name='question' and @disabled and @value='question0']\") assert html.xpath(\"//input[@name='answer' and @disabled and @value='answer0']\") The most important part of this test is the mocker fixture. We use it to replace the ollama.generate function with a dummy function that always returns the same answer. Setting up the LLM is far too costly for integration tests, which is why we mock it. Additionally, the response of an LLM is not deterministic, making it hard to test.\nThe test now sends a POST request to the /ask route with the question. The response is then checked for the appropriate status code and the expected HTML fragment. We parse the fragment with the help of lxml to run XPath queries against it. XPath is a query language for selecting nodes in an XML/HTML document. In our case, we use it to check if the input fields contain the expected values and are disabled.\nNext, we will look at a test for the /history route:\ndef test_history_empty(client): response = client.get(\"/history\") assert response.status_code == 200 html = lxml.html.fromstring(response.text) assert html.xpath(\"//table\") assert html.xpath(\"//table/thead/tr/th[text()='Question']\") assert html.xpath(\"//table/thead/tr/th[text()='Asked at']\") assert len(html.xpath(\"//table/tbody/tr\")) == 0 This test sends a GET request to retrieve the table of the last ten questions. In this case, there are no questions in the database, so the table should be empty. We check if the returned fragment is still a table with the correct headers. Additionally, we check if the table body is empty by asserting that an XPath query for table rows returns no results.\nYou can find the test_app module with all integration tests here. They are marked with the integration marker, so you can run them with the following command:\npoe integration XPath queries offer a concise way to check the rendered HTML, but aren’t the easiest to write. Fortunately, coding assistants, like GitHub Copilot, are proficient at generating XPaths from natural language. For integration tests, using XPaths is preferable over regexes or string comparisons, as they’re more robust and easier to read.\nEnd-to-End Tests for the Dashboard End-to-end tests run the same way a user would experience the dashboard: through the browser. Therefore, we need a way to automate browser interactions, which is the Python API of Playwright in our case. As the browser needs to communicate with the server, we need to set it up first. The following fixture will do so:\n@pytest.fixture def server(clean_database, setup_server): return setup_server @pytest.fixture(scope=\"module\") def setup_server(tmp_database): process = multiprocessing.Process( target=_setup_server, args=(\"app.main:app\",), kwargs={\"host\": \"localhost\", \"port\": 5001, \"database_url\": tmp_database}, daemon=True, ) process.start() for i in range(50): # 5-second timeout sleep(0.1) try: requests.get(\"http://localhost:5001\") except requests.ConnectionError: continue else: break else: raise TimeoutError(\"Server did not start in time\") yield process process.terminate() The setup_server fixture starts the dashboard server in a separate process. The _setup_server function is executed in the new process and sets the necessary environment variable for the database URL. After the process is started, we wait for the server to respond to a request with a timeout of five seconds. This ensures that the server is actually running before we continue with the tests. As before, we use the clean_database fixture to set up the database for the server. Instead of restarting the server for each test, we only start it once per test module. The clean_database fixture cleans up the database while the server keeps running. This saves valuable test time and still provides an independent environment for each test. This only works as long as the dashboard itself is stateless. If the dashboard maintains any kind of global state, we would need to restart the server for each test.\nThe following test checks if we receive an answer from the LLM when we ask a question:\n@pytest.mark.e2e def test_ask_question(server, page): page.goto(\"http://localhost:5001\") page.get_by_label(\"Question\").type(\"How are you doing?\") page.get_by_text(\"Ask\").click() page.wait_for_selector(\"input[name='answer']\") assert page.get_by_label(\"Question\").input_value() == \"How are you doing?\" assert page.get_by_label(\"Answer\").input_value() assert page.get_by_text(\"Reset\").is_visible() The page fixture is provided by Playwright and represents the browser page. We use it to navigate to our dashboard and interact with it as a user would. We locate the question input field by its label, type a question, and click the button labeled “Ask.” We then wait for the answer field to appear. Finally, we check if the question field still contains the question, the answer field is filled, and the reset button is visible.\nThis test is very brittle. It relies on the labels of the HTML elements to locate them and will time out if the LLM takes too long to respond. This is typical for end-to-end tests, which is why they may need to be updated more frequently. If the labels change, the test will fail, even though the functionality is still working. This brittleness is why end-to-end tests are the fewest in your test suite and are only run after the unit and integration tests pass.\nOn the other hand, these tests most closely align with the user experience. If the test fails due to a label change, the user would also be confused, and may need to be notified of this change. If the test times out due to the LLM, the user may also find the delay too long. This way, end-to-end tests provide a distinct kind of feedback, compared to the other test types.\nYou can find the test_app module with all end-to-end tests here. They are marked with the e2e marker, so you can run them with the following command:\npoe setup_e2e # installs chromium, ollama, and pulls the LLM poe e2e Running Automatically Automated tests are most useful, when they’re triggered to run automatically. Including them in pipelines offered by services like GitHub Actions or GitLab CI is standard practice for many teams. You can find an example pipeline for GitHub Actions here. It is configured to run the unit, integration, and end-to-end tests in this order on every push to the main branch. The latter types of tests are only run if the former pass, which saves time and costs.\nIt is best practice to cache dependencies in these pipelines. Not only does this speed up the pipeline execution, it alleviates the load on the package repositories which are often maintained by non-profit organizations. In our case, we cache the Python dependencies, the LLM model files, and the Playwright browser binaries.\nConclusion Automated testing is essential for developing software. Testing right is hard. Nevertheless, I hope this post is helpful for all the anxious data people out there who want to make sure their FastHTML dashboards run well. Again, you can find the whole code for this post here.\nIf you learned something, share this post with your peers. If not, please get back to me on how to improve the outlined testing process. I’m still on an ongoing testing journey and appreciate feedback, especially on the integration tests.\n","wordCount":"3009","inLanguage":"en","datePublished":"2024-10-15T00:00:00Z","dateModified":"2024-10-15T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tilman151.github.io/posts/testing-fasthtml/"},"publisher":{"@type":"Organization","name":"Don't Repeat Yourself","logo":{"@type":"ImageObject","url":"https://tilman151.github.io/img/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tilman151.github.io/ accesskey=h title="Don't Repeat Yourself (Alt + H)">Don't Repeat Yourself</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tilman151.github.io/publications/ title=Publications><span>Publications</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Testing FastHTML Dashboards</h1><div class=post-meta><span title='2024-10-15 00:00:00 +0000 UTC'>15 October 2024</span>&nbsp;·&nbsp;15 min</div></header><div class=post-content><p>Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.).
Often, these dashboards are hacked together in record time to meet a presentation deadline.
Now imagine this: you built a dashboard for showcasing your latest model to your team.
Instead of your go-to tool, <a href=https://streamlit.io/>Streamlit</a>, you decided to try out <a href=https://fastht.ml>FastHTML</a>, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger.
Your team lead is so impressed with your model that they want to show it to the whole company.
That is your chance to shine!
With FastHTML, you don&rsquo;t have to worry about scaling to a bigger audience.
But wait: are you sure your dashboard is really working as expected?
How can you be certain nothing fails if the CEO happens to use it?
Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing.</p><p>Thank goodness you found this post!
As I was in a similar situation and came up empty-handed, I needed to figure it out myself.
This post is intended for data people who want to test their FastHTML dashboards.
If you&rsquo;re a web developer, you might find this post too basic, but feel free to give feedback on how to improve the testing process described here.
But first, we need to go over some basics of automated testing.</p><h2 id=types-of-automated-tests>Types of Automated Tests<a hidden class=anchor aria-hidden=true href=#types-of-automated-tests>#</a></h2><p>There are several types of automated tests and even more definitions to each.
We will focus on three types here: unit tests, integration tests, and end-to-end tests.
There are many a StackOverflow post discussing where to draw the line between these types.
I was always of the opinion that one should be pragmatic about it, as long as the following is true:</p><blockquote><p>Unit tests are extremely fast, integration tests are reasonably fast, and end-to-end tests don&rsquo;t take forever to run.</p></blockquote><p>Each test of any type needs to be independent of the others, so you can run them in any order.
Furthermore, your test suite should have a lot of unit tests, fewer integration tests, and even fewer end-to-end tests.
We will explore why in the following sections.</p><h3 id=unit-tests>Unit Tests<a hidden class=anchor aria-hidden=true href=#unit-tests>#</a></h3><p>The term unit test is often synonymous with automatic test.
To be precise, unit tests check the functionality of the smallest code part, usually a function or a method.
They need next to no setup, don&rsquo;t interact with code that is not under test, and run lightning-fast.
Try to check all edge cases, so you don&rsquo;t have to do it with the other test types.
If you develop test-driven, these tests would be written in advance to define the functionality of your code.
But, as only a few people have the discipline to do so, unit tests are often written after the fact, too.
Because they&rsquo;re so fast, you can, in theory, run them every time you save your code.
Unit tests are the most numerous in your suite, because the smallest code parts are the most numerous in your codebase.</p><h3 id=integration-tests>Integration Tests<a hidden class=anchor aria-hidden=true href=#integration-tests>#</a></h3><p>Integrations tests check if the parts of your code integrate well with each other.
In our case, this could be the interaction between the dashboard endpoints and the database code.
They need some setup to ensure that the parts under test are connected as they would be in production.
Integration tests are a little slower than unit tests, but still fast enough to run them when you push to remote.
They&rsquo;re fewer than unit tests, because there should be fewer combinations in your codebase than individual code parts.
How to make sure this is the case is a topic for another post.</p><h3 id=end-to-end-tests>End-to-End Tests<a hidden class=anchor aria-hidden=true href=#end-to-end-tests>#</a></h3><p>End-to-end tests check if a user interaction can be successfully executed.
They&rsquo;re often mapped to user stories like &ldquo;<em>As a user, I want to log in to the dashboard and expect to see the start page afterward.</em>&rdquo;
These tests are the slowest of the bunch, because they need to set up the whole application before running.
End-to-end tests are the fewest in your suite, because they&rsquo;re the most complex to write and maintain.
They&rsquo;re also the most brittle, tending to break over minor changes like moving or renaming a button.</p><h2 id=our-example-dashboard>Our Example Dashboard<a hidden class=anchor aria-hidden=true href=#our-example-dashboard>#</a></h2><p>We will explore our test strategy with a <a href=https://github.com/tilman151/testing-fasthtml>simple example dashboard</a>.
It shows a text input field and a button labeled &ldquo;<em>Ask.</em>&rdquo;
When the user enters a question in the input field and clicks the button, the dashboard disables the input field and displays the LLM-generated answer below with a button to reset the dashboard.
The dashboard also features a button that displays the last ten questions asked which are stored in a Sqlite database.
This is as simple as it gets, but still provides ample opportunity for testing.
And of course, we make our lives more complicated than necessary by including LLM functionality.
It is 2024, after all.</p><p>We use <a href=https://github.com/astral-sh/uv>uv</a> for managing dependencies, <a href=https://www.sqlalchemy.org/>SQLAlchemy</a> for interacting with the database, <a href=https://docs.pytest.org/en/stable/>pytest</a> as our testing framework, and <a href=https://poethepoet.natn.io/>Poe the Poet</a> as our task runner.
For instructions on how to set up the project, refer to the <a href=https://github.com/tilman151/testing-fasthtml/blob/main/README.md>README</a>.</p><h3 id=unit-tests-for-the-database>Unit Tests for the Database<a hidden class=anchor aria-hidden=true href=#unit-tests-for-the-database>#</a></h3><p>First, we want to test the code that reads and writes the questions from and to the database.
I already hear the first people scoffing, &ldquo;<em>But that&rsquo;s not a unit test, that&rsquo;s an integration test!</em>&rdquo;
As I said, let us be pragmatic about it.
Testing database interaction code without a database is nearly useless.
Additionally, we&rsquo;re using Sqlite, which is an embedded database and part of the Python standard library.
You would not consider testing code that uses NumPy an integration test because it includes a C library, would you?</p><p>With that out of the way, there is some setup we need to do.
We need an empty database in working order before each test.
Remember, automated tests need to be independent, so the remains of the last test need to be removed from the database.
We use the following <a href=https://docs.pytest.org/en/7.1.x/explanation/fixtures.html>fixtures</a> for this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>clean_database</span>(tmp_database):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> tmp_database
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    engine <span style=color:#f92672>=</span> sa<span style=color:#f92672>.</span>create_engine(tmp_database)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> engine<span style=color:#f92672>.</span>connect() <span style=color:#66d9ef>as</span> connection:
</span></span><span style=display:flex><span>        database<span style=color:#f92672>.</span>metadata<span style=color:#f92672>.</span>drop_all(connection)
</span></span><span style=display:flex><span>        connection<span style=color:#f92672>.</span>commit()
</span></span><span style=display:flex><span>    _init_database(engine)
</span></span><span style=display:flex><span>    engine<span style=color:#f92672>.</span>dispose()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>(scope<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;module&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tmp_database</span>(tmp_database_path):
</span></span><span style=display:flex><span>    engine <span style=color:#f92672>=</span> sa<span style=color:#f92672>.</span>create_engine(tmp_database_path)
</span></span><span style=display:flex><span>    _init_database(engine)
</span></span><span style=display:flex><span>    engine<span style=color:#f92672>.</span>dispose()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tmp_database_path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>(scope<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;module&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tmp_database_path</span>(tmp_path_factory, request):
</span></span><span style=display:flex><span>    path <span style=color:#f92672>=</span> tmp_path_factory<span style=color:#f92672>.</span>mktemp(request<span style=color:#f92672>.</span>module<span style=color:#f92672>.</span>__name__ <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_data&#34;</span>, numbered<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    path <span style=color:#f92672>=</span> path <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;inventory.db&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;sqlite:///</span><span style=color:#e6db74>{</span>path<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><p>Let us start from the bottom:</p><ol><li>The <code>tmp_database_path</code> fixture creates a connection string for our database, pointing to a temporary directory.</li><li>The <code>tmp_database</code> fixture takes the connection string, creates the database, and initializes it with the <code>_init_database</code> function, which is omitted for brevity.</li><li>The <code>clean_database</code> fixture takes the connection string to the initialized database, yields it to the test function and cleans up afterward by reinitializing the database.</li></ol><p>But why so complicated?
Why not do it in one fixture?
The answer is saving time.
If we create a new database for each test, we would waste some time, slowing down our unit tests.
Remember, they need to be extremely fast.
By scoping <code>tmp_database_path</code> and <code>tmp_database</code> to module level, pytest runs these fixtures only once per test module.
This way, the database is only set up once and <code>clean_database</code> merely reinitializes it.</p><p>The last piece of setup concerns the way we connect to the database itself.
The <code>database</code> module of the dashboard maintains a global instance of an SQLAlchemy engine that controls the database connections.
This way, we can spin up connections to the database quickly without needing to establish the engine first.
Unfortunately, this also introduces a global state into our app.
Global state is always complicated for automated testing, as it breaks test independence.
The following fixture solves this problem for us:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>database_module</span>():
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> app <span style=color:#f92672>import</span> database
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    importlib<span style=color:#f92672>.</span>reload(database)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> database
</span></span></code></pre></div><p>This fixture supplies a freshly imported database module for each of our tests, effectively resetting the global state.</p><p>Now for the unit tests themselves.
We will exemplify the process on the <code>append_to_history</code> function.
Here is the test code:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_append_to_history</span>(database_module, clean_database):
</span></span><span style=display:flex><span>    database_module<span style=color:#f92672>.</span>connect(clean_database)
</span></span><span style=display:flex><span>    database_module<span style=color:#f92672>.</span>append_to_history(<span style=color:#e6db74>&#34;question0&#34;</span>, <span style=color:#e6db74>&#34;answer0&#34;</span>)
</span></span><span style=display:flex><span>    database_module<span style=color:#f92672>.</span>append_to_history(<span style=color:#e6db74>&#34;question1&#34;</span>, <span style=color:#e6db74>&#34;answer1&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> database_module<span style=color:#f92672>.</span>_get_connection() <span style=color:#66d9ef>as</span> conn:
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> conn<span style=color:#f92672>.</span>execute(database_module<span style=color:#f92672>.</span>history<span style=color:#f92672>.</span>select())<span style=color:#f92672>.</span>fetchall()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(result) <span style=color:#f92672>==</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, row <span style=color:#f92672>in</span> enumerate(result):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> row<span style=color:#f92672>.</span>question <span style=color:#f92672>==</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;question</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> row<span style=color:#f92672>.</span>answer <span style=color:#f92672>==</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;answer</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> isinstance(row<span style=color:#f92672>.</span>created_at, datetime<span style=color:#f92672>.</span>datetime)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> result[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>created_at <span style=color:#f92672>&lt;=</span> result[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>created_at
</span></span></code></pre></div><p>The function under test takes a question and the corresponding answer and inserts it into the history table.
The database is expected to supply the current timestamp for the <code>created_at</code> column.
With our test function, we first connect to the database, insert two lines of data, and retrieve them again.
We check if the table really contains only these two entries and if the <code>created_at</code> column was filled correctly.
Two fundamental unit testing practices can be observed here:</p><ol><li><strong>Fail for the right reason:</strong> unit tests are not only intended to check correctness, but also to hint in the direction of a bug. Whether there is only one row in the table, the rows are swapped or the <code>created_at</code> column is unfilled, each broken assumption fails on the appropriate <code>assert</code>. This makes debugging much easier.</li><li><strong>Test only one unit of code:</strong> even though, there is a function to retrieve questions from the database in our module, we don&rsquo;t use it here. If we use it, we would introduce another piece of code under test, making this an integration test.</li></ol><p>The <code>test_database</code> module contains more tests than this one.
You can find the complete module <a href=https://github.com/tilman151/testing-fasthtml/blob/main/tests/test_database.py>here</a>.
The tests can be run in the activated virtual environment with the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>poe unit
</span></span></code></pre></div><h3 id=integration-tests-for-the-routes>Integration Tests for the Routes<a hidden class=anchor aria-hidden=true href=#integration-tests-for-the-routes>#</a></h3><p><em>UPDATE: in a previous version of this article, the integration tests made use of regexes.
This was brittle and did a poor job of conveying the test&rsquo;s intent.
Thanks to Jeremy Howard&rsquo;s feedback, the tests now use the much more suitable XPath functionality of the lxml package.</em></p><p>Next, we will test if sending requests to the routes of our dashboard works as expected.
We will use the <code>starlette.testclient.TestClient</code> for this.
It allows us to simulate HTTP requests to our dashboard without spinning up a server.
The following fixture sets up the test client:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>client</span>(clean_database, monkeypatch):
</span></span><span style=display:flex><span>    monkeypatch<span style=color:#f92672>.</span>setenv(<span style=color:#e6db74>&#34;APP_DATABASE_URL&#34;</span>, clean_database)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> app.main <span style=color:#f92672>import</span> app
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    client <span style=color:#f92672>=</span> TestClient(app)
</span></span><span style=display:flex><span>    client<span style=color:#f92672>.</span>headers[<span style=color:#e6db74>&#34;HX-Request&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;true&#34;</span>
</span></span><span style=display:flex><span>    database<span style=color:#f92672>.</span>connect(clean_database)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> client
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    database<span style=color:#f92672>.</span>disconnect()
</span></span></code></pre></div><p>The <code>monkeypatch</code> fixture is used to temporarily set the database URL environment variable.
This way, we can connect the client to the database we set up in the <code>clean_database</code> fixture.
As you can see, we&rsquo;re using our <code>database.connect</code> and <code>database.disconnect</code> functions.
This is possible because integration tests are only executed once all unit tests have passed.
We can, therefore, assume that these functions are working as intended.
This demonstrates the importance of the test hierarchy, as we can now use our high-level functions in the integration tests, making them much more concise.
The <code>client</code> fixture also sets the <code>HX-Request</code> header to <code>true</code>.
This tells FastHTML that the request is coming from HTMX and the result should be an HTML fragment.
Otherwise, FastHTML would wrap the result in additional HTML tags, which would complicate the checks.</p><p>Let us first look at a test for the <code>/ask</code> route.
It expects a POST request with a JSON body containing a question.
The returned fragment is intended to be swapped with the question input field and submit button.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_ask_mocked_answer</span>(client, mocker):
</span></span><span style=display:flex><span>    mock_generate <span style=color:#f92672>=</span> mocker<span style=color:#f92672>.</span>patch(<span style=color:#e6db74>&#34;ollama.generate&#34;</span>)
</span></span><span style=display:flex><span>    mock_generate<span style=color:#f92672>.</span>return_value <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;response&#34;</span>: <span style=color:#e6db74>&#34;answer0&#34;</span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>post(<span style=color:#e6db74>&#34;/ask&#34;</span>, data<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;question&#34;</span>: <span style=color:#e6db74>&#34;question0&#34;</span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> response<span style=color:#f92672>.</span>status_code <span style=color:#f92672>==</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span>    html <span style=color:#f92672>=</span> lxml<span style=color:#f92672>.</span>html<span style=color:#f92672>.</span>fromstring(response<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> html<span style=color:#f92672>.</span>xpath(<span style=color:#e6db74>&#34;//input[@name=&#39;question&#39; and @disabled and @value=&#39;question0&#39;]&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> html<span style=color:#f92672>.</span>xpath(<span style=color:#e6db74>&#34;//input[@name=&#39;answer&#39; and @disabled and @value=&#39;answer0&#39;]&#34;</span>)
</span></span></code></pre></div><p>The most important part of this test is the <code>mocker</code> fixture.
We use it to replace the <code>ollama.generate</code> function with a dummy function that always returns the same answer.
Setting up the LLM is far too costly for integration tests, which is why we mock it.
Additionally, the response of an LLM is not deterministic, making it hard to test.</p><p>The test now sends a POST request to the <code>/ask</code> route with the question.
The response is then checked for the appropriate status code and the expected HTML fragment.
We parse the fragment with the help of <code>lxml</code> to run XPath queries against it.
<a href=https://en.wikipedia.org/wiki/XPath>XPath</a> is a query language for selecting nodes in an XML/HTML document.
In our case, we use it to check if the input fields contain the expected values and are disabled.</p><p>Next, we will look at a test for the <code>/history</code> route:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_history_empty</span>(client):
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> client<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;/history&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> response<span style=color:#f92672>.</span>status_code <span style=color:#f92672>==</span> <span style=color:#ae81ff>200</span>
</span></span><span style=display:flex><span>    html <span style=color:#f92672>=</span> lxml<span style=color:#f92672>.</span>html<span style=color:#f92672>.</span>fromstring(response<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> html<span style=color:#f92672>.</span>xpath(<span style=color:#e6db74>&#34;//table&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> html<span style=color:#f92672>.</span>xpath(<span style=color:#e6db74>&#34;//table/thead/tr/th[text()=&#39;Question&#39;]&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> html<span style=color:#f92672>.</span>xpath(<span style=color:#e6db74>&#34;//table/thead/tr/th[text()=&#39;Asked at&#39;]&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> len(html<span style=color:#f92672>.</span>xpath(<span style=color:#e6db74>&#34;//table/tbody/tr&#34;</span>)) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>
</span></span></code></pre></div><p>This test sends a GET request to retrieve the table of the last ten questions.
In this case, there are no questions in the database, so the table should be empty.
We check if the returned fragment is still a table with the correct headers.
Additionally, we check if the table body is empty by asserting that an XPath query for table rows returns no results.</p><p>You can find the <code>test_app</code> module with all integration tests <a href=https://github.com/tilman151/testing-fasthtml/blob/main/tests/test_app.py>here</a>.
They are marked with the <code>integration</code> marker, so you can run them with the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>poe integration
</span></span></code></pre></div><p>XPath queries offer a concise way to check the rendered HTML, but aren&rsquo;t the easiest to write.
Fortunately, coding assistants, like GitHub Copilot, are proficient at generating XPaths from natural language.
For integration tests, using XPaths is preferable over regexes or string comparisons, as they&rsquo;re more robust and easier to read.</p><h3 id=end-to-end-tests-for-the-dashboard>End-to-End Tests for the Dashboard<a hidden class=anchor aria-hidden=true href=#end-to-end-tests-for-the-dashboard>#</a></h3><p>End-to-end tests run the same way a user would experience the dashboard: through the browser.
Therefore, we need a way to automate browser interactions, which is the Python API of <a href=https://playwright.dev/python/>Playwright</a> in our case.
As the browser needs to communicate with the server, we need to set it up first.
The following fixture will do so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>server</span>(clean_database, setup_server):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> setup_server
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>@pytest.fixture</span>(scope<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;module&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>setup_server</span>(tmp_database):
</span></span><span style=display:flex><span>    process <span style=color:#f92672>=</span> multiprocessing<span style=color:#f92672>.</span>Process(
</span></span><span style=display:flex><span>        target<span style=color:#f92672>=</span>_setup_server,
</span></span><span style=display:flex><span>        args<span style=color:#f92672>=</span>(<span style=color:#e6db74>&#34;app.main:app&#34;</span>,),
</span></span><span style=display:flex><span>        kwargs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;host&#34;</span>: <span style=color:#e6db74>&#34;localhost&#34;</span>, <span style=color:#e6db74>&#34;port&#34;</span>: <span style=color:#ae81ff>5001</span>, <span style=color:#e6db74>&#34;database_url&#34;</span>: tmp_database},
</span></span><span style=display:flex><span>        daemon<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    process<span style=color:#f92672>.</span>start()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>50</span>):  <span style=color:#75715e># 5-second timeout</span>
</span></span><span style=display:flex><span>        sleep(<span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            requests<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#34;http://localhost:5001&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span> requests<span style=color:#f92672>.</span>ConnectionError:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>TimeoutError</span>(<span style=color:#e6db74>&#34;Server did not start in time&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>yield</span> process
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    process<span style=color:#f92672>.</span>terminate()
</span></span></code></pre></div><p>The <code>setup_server</code> fixture starts the dashboard server in a separate process.
The <code>_setup_server</code> function is executed in the new process and sets the necessary environment variable for the database URL.
After the process is started, we wait for the server to respond to a request with a timeout of five seconds.
This ensures that the server is actually running before we continue with the tests.
As before, we use the <code>clean_database</code> fixture to set up the database for the server.
Instead of restarting the server for each test, we only start it once per test module.
The <code>clean_database</code> fixture cleans up the database while the server keeps running.
This saves valuable test time and still provides an independent environment for each test.
This only works as long as the dashboard itself is stateless.
If the dashboard maintains any kind of global state, we would need to restart the server for each test.</p><p>The following test checks if we receive an answer from the LLM when we ask a question:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@pytest.mark.e2e</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_ask_question</span>(server, page):
</span></span><span style=display:flex><span>    page<span style=color:#f92672>.</span>goto(<span style=color:#e6db74>&#34;http://localhost:5001&#34;</span>)
</span></span><span style=display:flex><span>    page<span style=color:#f92672>.</span>get_by_label(<span style=color:#e6db74>&#34;Question&#34;</span>)<span style=color:#f92672>.</span>type(<span style=color:#e6db74>&#34;How are you doing?&#34;</span>)
</span></span><span style=display:flex><span>    page<span style=color:#f92672>.</span>get_by_text(<span style=color:#e6db74>&#34;Ask&#34;</span>)<span style=color:#f92672>.</span>click()
</span></span><span style=display:flex><span>    page<span style=color:#f92672>.</span>wait_for_selector(<span style=color:#e6db74>&#34;input[name=&#39;answer&#39;]&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> page<span style=color:#f92672>.</span>get_by_label(<span style=color:#e6db74>&#34;Question&#34;</span>)<span style=color:#f92672>.</span>input_value() <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;How are you doing?&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> page<span style=color:#f92672>.</span>get_by_label(<span style=color:#e6db74>&#34;Answer&#34;</span>)<span style=color:#f92672>.</span>input_value()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> page<span style=color:#f92672>.</span>get_by_text(<span style=color:#e6db74>&#34;Reset&#34;</span>)<span style=color:#f92672>.</span>is_visible()
</span></span></code></pre></div><p>The <code>page</code> fixture is provided by Playwright and represents the browser page.
We use it to navigate to our dashboard and interact with it as a user would.
We locate the question input field by its label, type a question, and click the button labeled &ldquo;Ask.&rdquo;
We then wait for the answer field to appear.
Finally, we check if the question field still contains the question, the answer field is filled, and the reset button is visible.</p><p>This test is very brittle.
It relies on the labels of the HTML elements to locate them and will time out if the LLM takes too long to respond.
This is typical for end-to-end tests, which is why they may need to be updated more frequently.
If the labels change, the test will fail, even though the functionality is still working.
This brittleness is why end-to-end tests are the fewest in your test suite and are only run after the unit and integration tests pass.</p><p>On the other hand, these tests most closely align with the user experience.
If the test fails due to a label change, the user would also be confused, and may need to be notified of this change.
If the test times out due to the LLM, the user may also find the delay too long.
This way, end-to-end tests provide a distinct kind of feedback, compared to the other test types.</p><p>You can find the <code>test_app</code> module with all end-to-end tests <a href=https://github.com/tilman151/testing-fasthtml/blob/main/tests/test_app.py>here</a>.
They are marked with the <code>e2e</code> marker, so you can run them with the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>poe setup_e2e  <span style=color:#75715e># installs chromium, ollama, and pulls the LLM</span>
</span></span><span style=display:flex><span>poe e2e
</span></span></code></pre></div><h2 id=running-automatically>Running Automatically<a hidden class=anchor aria-hidden=true href=#running-automatically>#</a></h2><p>Automated tests are most useful, when they&rsquo;re triggered to run automatically.
Including them in pipelines offered by services like GitHub Actions or GitLab CI is standard practice for many teams.
You can find an example pipeline for GitHub Actions <a href=https://github.com/tilman151/testing-fasthtml/blob/main/.github/workflows/on_push.yaml>here</a>.
It is configured to run the unit, integration, and end-to-end tests in this order on every push to the main branch.
The latter types of tests are only run if the former pass, which saves time and costs.</p><p>It is best practice to cache dependencies in these pipelines.
Not only does this speed up the pipeline execution, it alleviates the load on the package repositories which are often maintained by non-profit organizations.
In our case, we cache the Python dependencies, the LLM model files, and the Playwright browser binaries.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Automated testing is essential for developing software.
Testing right is hard.
Nevertheless, I hope this post is helpful for all the anxious data people out there who want to make sure their FastHTML dashboards run well.
Again, you can find the whole code for this post <a href=https://github.com/tilman151/testing-fasthtml>here</a>.</p><p>If you learned something, share this post with your peers.
If not, please get back to me on how to improve the outlined testing process.
I&rsquo;m still on an ongoing testing journey and appreciate feedback, especially on the integration tests.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tilman151.github.io/tags/fasthtml/>Fasthtml</a></li><li><a href=https://tilman151.github.io/tags/testing/>Testing</a></li><li><a href=https://tilman151.github.io/tags/cleancode/>Cleancode</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tilman151.github.io/>Don't Repeat Yourself</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>#cookie-notice{padding:1.5rem 1rem;display:none;text-align:center;position:fixed;bottom:0;width:100%;background:var(--border);color:var(--primary)}#cookie-notice a{display:inline-block;cursor:pointer;margin-left:1rem}@media(max-width:767px){#cookie-notice span{display:block;padding-top:3px;margin-bottom:1rem}#cookie-notice a{position:relative;bottom:4px}}</style><div id=cookie-notice><span>I am using third-party cookies to count readers. If you don't want to be counted, please use incognito mode.</span><a id=cookie-notice-accept class="btn btn-primary btn-sm">OK</a><a href=/privacy class="btn btn-primary btn-sm">More info</a></div><script>function createCookie(e,t,n){var s,o="";n&&(s=new Date,s.setTime(s.getTime()+n*24*60*60*1e3),o="; expires="+s.toUTCString()),document.cookie=e+"="+t+o+"; path=/"}function readCookie(e){for(var t,s=e+"=",o=document.cookie.split(";"),n=0;n<o.length;n++){for(t=o[n];t.charAt(0)==" ";)t=t.substring(1,t.length);if(t.indexOf(s)==0)return t.substring(s.length,t.length)}return null}function eraseCookie(e){createCookie(e,"",-1)}if(readCookie("cookie-notice-dismissed")=="true"){var s,clicky_site_ids=clicky_site_ids||[];clicky_site_ids.push(101271854),s=document.createElement("script"),s.src="https://static.getclicky.com/js",document.head.appendChild(s)}else document.getElementById("cookie-notice").style.display="block";document.getElementById("cookie-notice-accept").addEventListener("click",function(){createCookie("cookie-notice-dismissed","true",31),document.getElementById("cookie-notice").style.display="none",location.reload()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>