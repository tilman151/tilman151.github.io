<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Don't Repeat Yourself</title><link>https://krokotsch.eu/posts/</link><description>Recent content in Posts on Don't Repeat Yourself</description><generator>Hugo -- 0.155.3</generator><language>en-us</language><lastBuildDate>Sun, 08 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://krokotsch.eu/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Letting AI Write a Newsletter with n8n</title><link>https://krokotsch.eu/posts/ai-newsletter-with-n8n/</link><pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/ai-newsletter-with-n8n/</guid><description>&lt;p&gt;Iâ€™ve been a member of an orchestra in my hometown since childhood.
Although I only get to play my chosen instrument a few times each year, I&amp;rsquo;m still active in the orchestra&amp;rsquo;s board.
Due to my profession, most things that can be done digitally fall into my domain.
I do not particularly enjoy these administrative tasks, but somebody has to do them.
One of these tasks is especially bothersome: &lt;em&gt;writing the weekly newsletter&lt;/em&gt;.
This newsletter is an e-mail with all important dates for the week, reminders, and an outlook of events in the next few weeks.
The newsletter is also cross-posted into the orchestra&amp;rsquo;s WhatsApp group chat.
Compiling this newsletter means checking several sources for information and bringing them together in a few well-written paragraphs.&lt;/p&gt;</description></item><item><title>Shape Up for Consulting Work</title><link>https://krokotsch.eu/posts/shape-up-consulting/</link><pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/shape-up-consulting/</guid><description>&lt;p&gt;During the holidays I found the time to read a surprisingly wonderful book: Shape Up by Rian Singer.
I came about it in a blog post about alternative engineering practices on &lt;a href="https://daily.dev"&gt;daily.dev&lt;/a&gt;, which I obviously forgot to bookmark and, therefore, cannot mention here (nevermind, &lt;a href="https://newsletter.manager.dev/p/5-engineering-dogmas-its-time-to"&gt;found it&lt;/a&gt;).
For the people who do not know Rian Singer (I was among them before reading the book), he is a product strategist at 37signals, the company that builds Basecamp.
The Shape Up book details the project management process of the same name used at 37signals.
As a longterm admirer of Basecamp, I was immediately intrigued.&lt;/p&gt;</description></item><item><title>Testing FastHTML Dashboards</title><link>https://krokotsch.eu/posts/testing-fasthtml/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/testing-fasthtml/</guid><description>&lt;p&gt;Building dashboards to visualize data or the results of experiments is the bread and butter of data people (read: data scientist, engineers, analysts, etc.).
Often, these dashboards are hacked together in record time to meet a presentation deadline.
Now imagine this: you built a dashboard for showcasing your latest model to your team.
Instead of your go-to tool, &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt;, you decided to try out &lt;a href="https://fastht.ml"&gt;FastHTML&lt;/a&gt;, a shiny new framework that promises better handling and scalability if your dashboard ever needs to go bigger.
Your team lead is so impressed with your model that they want to show it to the whole company.
That is your chance to shine!
With FastHTML, you don&amp;rsquo;t have to worry about scaling to a bigger audience.
But wait: are you sure your dashboard is really working as expected?
How can you be certain nothing fails if the CEO happens to use it?
Normally, you would go for automated testing, but after scouring the FastHTML documentation on how to do it, you found nothing.&lt;/p&gt;</description></item><item><title>Compose Datasets, Don't Inherit Them</title><link>https://krokotsch.eu/posts/compose-datasets/</link><pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/compose-datasets/</guid><description>&lt;p&gt;In relatively young disciplines, like deep learning, people tend to leave behind old principles.
Sometimes this is a good thing because times have changed and old truths, i.e. over-completeness being a bad thing, have to go.
Other times, such old principles stick around for a reason and still people over-eagerly try to throw them out of the window.
I am no exception in this regard so let me tell you how I &amp;ldquo;re-learned&amp;rdquo; the tried and true design pattern of &amp;ldquo;Composition over Inheritance&amp;rdquo;.&lt;/p&gt;</description></item><item><title>About Copying Blindly - Insights of the One-Eyed Person</title><link>https://krokotsch.eu/posts/about-copying-blindly/</link><pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/about-copying-blindly/</guid><description>&lt;p&gt;Today I have no fancy project and no shiny GitHub repository to show because today I want to talk about my research.
As some may read in my bio, I am doing my PhD in the field of predictive maintenance (PDM).
This field is directly adjacent to machine learning and fell, like many others, into the grasp of the deep learning hype.
Unfortunately, long series of sensor readings are not as interesting to look at as images or intuitively understood as natural language, so PDM is not as present in the mind of the general ML crowd.
Maybe this post can shed a little light on this corner of the research world.&lt;/p&gt;</description></item><item><title>The Great Autoencoder Bake Off</title><link>https://krokotsch.eu/posts/ae-bakeoff/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/ae-bakeoff/</guid><description>&lt;p&gt;&lt;em&gt;&amp;ldquo;Another article comparing types of autoencoders?&amp;rdquo;&lt;/em&gt;, you may think.
&lt;em&gt;&amp;ldquo;There are already so many of them!&amp;rdquo;&lt;/em&gt;, you may think.
&lt;em&gt;&amp;ldquo;How does he know what I am thinking?!&amp;rdquo;&lt;/em&gt;, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.&lt;/p&gt;
&lt;p&gt;There are indeed articles comparing some autoencoders to each other (e.g. &lt;a href="https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64"&gt;[1]&lt;/a&gt;, &lt;a href="https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924"&gt;[2]&lt;/a&gt;, &lt;a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"&gt;[3]&lt;/a&gt;), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this &lt;a href="https://github.com/AntixK/PyTorch-VAE"&gt;repository&lt;/a&gt; I found, I took it into my own hands, and thus this blog post came into existence.&lt;/p&gt;</description></item><item><title>Make DL4J Readable Again</title><link>https://krokotsch.eu/posts/kotlin-dsl-for-dl4j/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/kotlin-dsl-for-dl4j/</guid><description>&lt;p&gt;A while ago, I stumbled upon an &lt;a href="https://blog.jetbrains.com/kotlin/2019/12/making-kotlin-ready-for-data-science/"&gt;article&lt;/a&gt; about the language Kotlin and how to use it for Data Science.
I found it interesting, as some of Python&amp;rsquo;s quirks were starting to bother me and I wanted to try something new.
A day later, I had completed the Kotlin tutorials using &lt;a href="https://www.jetbrains.com/help/education/learner-start-guide.html?section=Kotlin%20Koans&amp;amp;_ga=2.101592385.1724296010.1598524435-160366776.1590830721"&gt;Kotlin Koans&lt;/a&gt; in IntelliJ IDEA (which is an excellent way to get started with Kotlin).
Hungry to test out my new language skills, I looked around for a project idea.
As I am a deep learning engineer, naturally I had a look at what DL frameworks Kotlin had to offer and arrived at DL4J.
This is actually a Java framework, but as Kotlin is interoperable with Java, it can be used anyway.
I had a look at some examples of how to build a network and found this (&lt;a href="https://github.com/eclipse/deeplearning4j-examples/blob/master/dl4j-examples/src/main/kotlin/org/deeplearning4j/quickstartexamples/feedforward/mnist/MLPMnistTwoLayerExample.kt"&gt;Source&lt;/a&gt;):&lt;/p&gt;</description></item><item><title>How to Trust Your Deep Learning Code</title><link>https://krokotsch.eu/posts/deep-learning-unit-tests/</link><pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/deep-learning-unit-tests/</guid><description>&lt;p&gt;Deep learning is a discipline where the correctness of code is hard to assess.
Random initialization, huge datasets and limited interpretability of weights mean that finding the exact issue of why your model is not training, is trial-and-error most times.
In classical software development, automated unit tests are the bread and butter for determining if your code does what it is supposed to do.
It helps the developer to trust their code and be confident when introducing changes.
A breaking change would be detected by the unit tests.&lt;/p&gt;</description></item></channel></rss>