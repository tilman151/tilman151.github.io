<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Great Autoencoder Bake Off | Don't Repeat Yourself</title>
<meta name=keywords content="deeplearning"><meta name=description content="&ldquo;Another article comparing types of autoencoders?&rdquo;, you may think.
&ldquo;There are already so many of them!&rdquo;, you may think.
&ldquo;How does he know what I am thinking?!&rdquo;, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.
There are indeed articles comparing some autoencoders to each other (e.g. [1], [2], [3]), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this repository I found, I took it into my own hands, and thus this blog post came into existence."><meta name=author content><link rel=canonical href=https://tilman151.github.io/posts/ae-bakeoff/><link crossorigin=anonymous href=/assets/css/stylesheet.5cb2fc31fd4c267a86f6f77cd180c44dd566ab953b1f23bd24d4ed27f19f73b4.css integrity="sha256-XLL8Mf1MJnqG9vd80YDETdVmq5U7HyO9JNTtJ/Gfc7Q=" rel="preload stylesheet" as=style><link rel=icon href=https://tilman151.github.io/img/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://tilman151.github.io/img/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://tilman151.github.io/img/favicon-32x32.png><link rel=apple-touch-icon href=https://tilman151.github.io/img/apple-touch-icon.png><link rel=mask-icon href=https://tilman151.github.io/img/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://tilman151.github.io/posts/ae-bakeoff/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:title" content="The Great Autoencoder Bake Off"><meta property="og:description" content="&ldquo;Another article comparing types of autoencoders?&rdquo;, you may think.
&ldquo;There are already so many of them!&rdquo;, you may think.
&ldquo;How does he know what I am thinking?!&rdquo;, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.
There are indeed articles comparing some autoencoders to each other (e.g. [1], [2], [3]), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this repository I found, I took it into my own hands, and thus this blog post came into existence."><meta property="og:type" content="article"><meta property="og:url" content="https://tilman151.github.io/posts/ae-bakeoff/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-01-24T00:00:00+00:00"><meta property="article:modified_time" content="2021-01-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Great Autoencoder Bake Off"><meta name=twitter:description content="&ldquo;Another article comparing types of autoencoders?&rdquo;, you may think.
&ldquo;There are already so many of them!&rdquo;, you may think.
&ldquo;How does he know what I am thinking?!&rdquo;, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.
There are indeed articles comparing some autoencoders to each other (e.g. [1], [2], [3]), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this repository I found, I took it into my own hands, and thus this blog post came into existence."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tilman151.github.io/posts/"},{"@type":"ListItem","position":2,"name":"The Great Autoencoder Bake Off","item":"https://tilman151.github.io/posts/ae-bakeoff/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Great Autoencoder Bake Off","name":"The Great Autoencoder Bake Off","description":"\u0026ldquo;Another article comparing types of autoencoders?\u0026rdquo;, you may think. \u0026ldquo;There are already so many of them!\u0026rdquo;, you may think. \u0026ldquo;How does he know what I am thinking?!\u0026rdquo;, you may think. While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.\nThere are indeed articles comparing some autoencoders to each other (e.g. [1], [2], [3]), I found them lacking something. Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do. Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks. I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one. Inspired by this repository I found, I took it into my own hands, and thus this blog post came into existence.\n","keywords":["deeplearning"],"articleBody":"“Another article comparing types of autoencoders?”, you may think. “There are already so many of them!”, you may think. “How does he know what I am thinking?!”, you may think. While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.\nThere are indeed articles comparing some autoencoders to each other (e.g. [1], [2], [3]), I found them lacking something. Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do. Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks. I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one. Inspired by this repository I found, I took it into my own hands, and thus this blog post came into existence.\nOf course, this article will probably not be a complete list of all the autoencoders out there, there are just too many. But, if you are missing your favorite autoencoder, feel free to contact me or send a PR. I tried to structure the code to be easily extendable, and you can find it at github.com/tilman151/ae_bakeoff.\nThis is my first personal project with pytorch-lightning, too. It really helped to make my code more structured, readable and cut out tons of boilerplate code. You can check out this great package here.\nWhat and how are we comparing? First, let’s make clear what the inclusion criterion for this article is. We are interested in all modifications to a vanilla deep autoencoder that change the way the latent space behaves or may improve a downstream task. This excludes application-specific losses (e.g. perceptual VGG19 loss for images) and the types of encoder and decoder (e.g. LSTM vs. CNN). The following types have made the cut:\nShallow Autoencoders Deep Autoencoders (vanilla AE) Stacked Autoencoders Sparse Autoencoders Denoising Autoencoders Variational Autoencoders (VAE) Beta Variational Autoencoders (beta-VAE) Vector-Quantized Variational Autoencoders (vq-VAE) Aside from describing what makes each of them unique, we will compare them in the following categories:\n(Update 03.03.2021) Training time Reconstruction quality Quality of decoded samples from the latent space (if possible) Quality of latent space interpolation Structure of the latent space visualized with UMAP ROC curve for anomaly detection with the reconstruction error Classification accuracy of a linear layer fitted on the autoencoder’s features All autoencoders tested share the same simple architecture with a fully-connected en- and decoder, batch normalization, and ReLU activation functions. The output layer features a sigmoid activation function. Save for the shallow one, all autoencoders have three encoder and decoder layers each.\nThe dimension of the latent space and the number of network parameters is held approximately constant. This means that a variational autoencoder will have more parameters than a vanilla one as the encoder has to produce $2n$ outputs for a latent space of dimension $n$. We make two training runs for each autoencoder: once with a latent space of 20 dimensions, and once with 2 dimensions. The model from the second training run is used for anomaly detection, the first one for all other tasks. Both variants are undercomplete, meaning that they have a higher input dimension than latent space dimension.\nThe MNIST dataset will serve as the venue for our contest. The default train/test split is further divided into a train/val/test split by taking a random sample of 5000 samples from the training split for validation.\nObviously, there are some limitations to what we are doing here. All training runs are only done once, so we have no measure of how stable the performances are. We are only using one dataset, so drawing any generalized conclusions is off-limits. Nevertheless, this is a blog and not a journal, so I think we are fine. With all that in mind, let’s begin our Great Autoencoder Bake Off.\nThe Contestants First, we will have a look at how each tested autoencoder works and what makes them special. From that, we will try to form a few hypotheses on how they will perform on the tasks.\nShallow Autoencoder The shallow autoencoder is not really a contestant as it is far too underpowered to keep up with the others. It is only here as a baseline.\n$$\\hat{x} = \\operatorname{dec}(\\operatorname{enc}(x))$$\nAbove you can see the reconstruction formula of the shallow autoencoder. The formula declares how an autoencoder reconstructs a sample $x$ in a semi-mathematical way.\nA shallow autoencoder features only one layer in its encoder and decoder each. To distinguish the shallow autoencoder from a PCA, it uses a ReLU activation function in the encoder and a sigmoid in the decoder. It is therefore non-linear.\nNormally, the default choice for the reconstruction loss for an autoencoder is mean squared error. We will use binary cross-entropy (BCE) because it yielded better-looking images in the preliminary experiments. If you want to know why this is a valid choice of a loss function, I recommend reading chapters 5.5 and 6.2.1 of the Deep Learning Book. As all following autoencoders, the shallow one is trained against the following version of BCE:\n$$\\mathcal{L}_r = \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^{h\\cdot w} \\operatorname{BCE}(\\hat{x}_j^{(i)}, x_j^{(i)})$$\nwhere $x^{(i)}_j$ is the $j$th pixel of the $i$th input image and $\\hat{x}^{(i)}_j$ the corresponding reconstruction. The loss is, hence, summed up per image, and averaged over the batch. This decision will become important for the variational autoencoder.\nDeep Autoencoder The deep autoencoder (a.k.a. vanilla autoencoder) is the big brother of the shallow one. They are basically the same, but the deep autoencoder has more layers. Therefore, they have the same reconstruction formula, too.\nThe deep autoencoder has no restrictions on its latent space and should, as a consequence, be able to encode the most information in it.\nStacked Autoencoder The stacked autoencoder is a “hack” to get a deep autoencoder by training only shallow ones. Instead of training the autoencoder end-to-end, we train it in a layer-wise, greedy fashion. First, we take the first encoder and last decoder layer to form a shallow autoencoder. After training these layers, we encode the whole dataset with the encoding layer and form another shallow autoencoder from the second encoder and next-to-last decoder layer. This second shallow autoencoder is trained with the encoded dataset. This process is repeated until we arrive at the innermost layers. In the end, we get a deep autoencoder out of stacked shallow autoencoders.\nWe will train each of the $n$ layers for $\\frac{1}{n}th$ of the total training epochs.\nAs the stacked autoencoder differs from a deep one only by training procedure, it has the same reconstruction function, too. Again, we have no restrictions on the latent space, but the encoding is expected to be worse due to the greedy training procedure.\nSparse Autoencoder The sparse autoencoder imposes a sparsity constraint on the latent code. Each element in the latent code should only be active with a probability $p$. We add the following auxiliary loss to enforce it while training:\n$$L_s(z) = \\sum_{i=1}^N \\left( p \\cdot \\log{\\frac{p}{\\bar{z}^{(i)}}} + (1 - p) \\cdot \\log{\\frac{1 - p}{1 - \\bar{z}^{(i)}}} \\right)$$\nwhere $\\bar{z}^{(i)}$ is the average activation of the $i$th element of the latent code over a batch. This loss corresponds to the sum of $|z|$ Kulback-Leibler divergences between binomial distributions with the means $p$ and $\\bar{z}^{(i)}$. Other implementations may be possible to enforce the sparsity constraint, too.\nTo make this sparsity loss possible, we have to scale the latent code to $[0, 1]$ in order to be interpreted as probabilities. This is done with a sigmoid activation function which gives us following reconstruction formula:\n$$\\hat{x} = \\operatorname{dec}(\\sigma(\\operatorname{enc}(x)))$$\nThe complete loss for the sparse autoencoder combines the reconstruction loss and sparsity loss with an influence hyperparameter $\\beta$:\n$$L = L_r + \\beta L_s$$\nWe will set $p$ to 0.25 and $\\beta$ to one for all experiments.\nDenoising Autoencoder The denoising autoencoder does not restrict the latent space but aims to learn a more efficient encoding through applying noise to the input data. Instead of feeding the input data straight to the network, we add Gaussian noise as follows:\n$$x’ = \\operatorname{clip} (x + \\mathcal{N}(0;\\operatorname{diag}(\\beta\\mathbf{I}))) $$\nwhere $\\operatorname{clip}$ is clipping its input to $[0, 1]$ and the scalar $\\beta$ is the variance of the noise. Hence, the autoencoder is trained on reconstructing clean samples from a noisy version. The reconstruction formula is:\n$$\\hat{x} = \\operatorname{dec}(\\operatorname{enc}(x’))$$\nWe use the noisy input exclusively in training. When evaluating the autoencoder, we feed it the original input data. Otherwise, the denoising autoencoder uses the same loss function as the ones before. For all experiments, $\\beta$ is set to 0.5.\nVariational Autoencoder In theory, the variational autoencoder (VAE) has not that much to do with a vanilla one. In practice, the implementation and training are very similar. The VAE interprets the reconstruction as a stochastic process, making it non-deterministic. The encoder does not output the latent code, but the parameters of a probability distribution of latent codes. The decoder then receives a sample from this distribution. The default choice of distribution family is a Gaussian $\\mathcal{N}(\\mu; \\operatorname{diag}(\\Sigma))$. The reconstruction formula looks as follows:\n$$\\hat{x} = \\operatorname{dec}(\\operatorname{sample}(\\operatorname{enc}_\\mu(x), \\operatorname{enc}_\\Sigma(x)))$$\nwhere $\\operatorname{enc}_\\mu(x)$ and $\\operatorname{enc}_\\Sigma(x)$ encode $x$ to $\\mu$ and $\\Sigma$. Both encoders share most of their parameters. In practice, a single encoder simply gets two output layers instead of one. The problem is now that sampling from a distribution is an operation without gradient and backpropagation from the decoder to the encoder would be impossible. The solution is called the reparametrization trick and transforms a sample from a standard Gaussian to one from a Gaussian parametrized by $\\mu$ and $\\Sigma$:\n$$\\operatorname{reparametrize}(\\mu, \\Sigma) = \\Sigma \\cdot \\operatorname{sample}(0, \\operatorname{diag}(\\mathbf{I})) + \\mu$$\nThis formulation has a gradient with respect to $\\mu$ and $\\Sigma$ and makes training with backpropagation possible.\nThe variational autoencoder further restricts the latent space by requiring Gaussian distributions to be similar to a standard Gaussian. The distribution parameters are therefore penalized with the Kulback-Leibler divergence:\n$$L_{KL} = \\frac{1}{2N}\\sum_{i=1}^N \\sum_{j=1}^{|z|} (2\\Sigma_j^{(i)} + (\\mu_j^{(i)})^2 - 1 - 2\\log{(\\Sigma_j^{(i)})})$$\nThe divergence is averaged over the batch. The reconstruction loss is, as mentioned before, averaged in the same way to retain the correct ratio between the reconstruction loss and the divergence loss. The full training loss is then:\n$$L = L_r + L_{KL}$$\nAs we try to constrain the encoder to output standard Gaussians, we can try to decode a sample from a standard Gaussian directly, too. This unconditioned sampling is a unique property of VAEs and makes them generative models, similar to GANs. The formula for unconditional sampling is:\n$$\\hat{x} = \\operatorname{dec}(\\operatorname{sample}(0, \\operatorname{diag}(\\mathbf{I}))$$\nIf you want to understand the theory behind VAEs, I recommend the tutorial by Carl Doersch.\nBeta Variational Autoencoder The beta VAE is a generalization of the VAE that simply changes the ratio between reconstruction and divergence loss. The influence of the divergence loss is parametrized by a scalar $\\beta$, hence the name:\n$$L = L_r + \\beta L_{KL}$$\nA $\\beta \u003c 1$ relaxes the constraints on the latent space while a $\\beta \u003e 1$ makes the constraint stricter. The former should result in better reconstructions and the latter in better unconditional sampling. The theoretic derivation makes good arguments about what else this simple change affects. You can read them on OpenReview. We will use a strict version of this autoencoder with $\\beta = 2$ and a loose version with $\\beta = 0.5$.\nVector-Quantized Variational Autoencoder The vector-quantized variational autoencoder (vq-VAE) is a VAE that uses a uniform categorical distribution to generate its latent codes. Each element of the encoder output is replaced by the categorical value of the distribution that is its nearest neighbor. This is a form of quantization and means that the latent space is not continuous anymore but discrete.\n$$\\hat{x} = \\operatorname{dec}(\\operatorname{quantize}(\\operatorname{enc}(x))))$$\nThe categories themselves are learned by minimizing the sum squared error to the encoder outputs:\n$$L_{vq} = \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^{|z|} (\\operatorname{sg}(z_j^{(i)}) - \\hat{z}_j^{(i)})^2$$\nwhere $z$ is the output of the encoder, $\\hat{z}$ is the corresponding quantized latent code, and $sg$ the stop gradient operator. On the other hand, the encoder is encouraged to output encodings similar to the categories by sum squared error, too:\n$$L_{c} = \\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^{|z|} (z_j^{(i)} - \\operatorname{sg}(\\hat{z}_j^{(i)}))^2$$\nThis is called the commitment loss. A Kulback-Leibler divergence loss is not necessary, as the divergence would be a constant for a uniform categorical distribution. Both losses are combined with the reconstruction loss by a hyperparameter $\\beta$ controlling the influence of the commitment loss:\n$$L = L_r + L_{vq} + \\beta L_c$$\nWe set $\\beta$ to one for all experiments.\nBecause the vq-VAE is a generative model, we can sample from it unconditionally, too. In the original paper, a pixel-CNN is used to autoregressively sample a latent code. For simplicity’s sake, we will sample by drawing categories uniformly.\nThe Gauntlet Now that we know who we are dealing with, let’s see how they fare in our gauntlet of tasks. Besides the standard ones like reconstruction quality, we will have a look at semi-supervised classification and anomaly detection, too.\nTraining Time (Update 03.03.2021) As per feedback from Reddit, we are going to look at the training times of the autoencoders. The time was not recorded directly because we would need to rerun the experiments to do that. Instead, we are using the TensorBoard event files and calculate the time between the first training loss and last validation loss logged. This gives us only an offset of the iteration time of the first training batch. The times should, therefore, be interpreted as the time to run 60 epochs with validation after each one.\nshallow vanilla stacked sparse denoising 0:10:38 0:11:15 0:11:05 0:11:27 0:11:18 vae beta_vae_strict beta_vae_loose vq 0:11:30 0:11:29 0:11:28 0:11:27 Unsurprisingly, there is not much of a difference. Even the shallow autoencoder, that is much smaller, takes only about half a minute less time. This indicates that the time for the forward and backward passes is much smaller than the overhead of data loading, logging, etc. Some variation can be attributed to this being computed on a Windows machine, too. Who knows what background process is hogging resources at any given time?\nReconstruction First and foremost, we want to see how well each autoencoder can reconstruct its input. For this, we can have a look at the 16 first images of the MNIST test set.\nshallow vanilla stacked sparse denoising vae beta_vae_strict beta_vae_loose vq The shallow autoencoder fails to reconstruct many of the test samples correctly. Fours and nines are barely distinguishable, and some digits are not recognizable at all. The other autoencoders do a much better, but not perfect, job. The denoising one is in the habit of making some thin lines disappear. The sparse and stacked ones seem to have problems with abnormally formed digits like the five and one of the nines. Overall the reconstructions are a little blurry, which is normal for autoencoders. The vq-VAE produces slightly less blurry reconstructions, which the original authors credit to the discrete latent space.\nOtherwise, there is little difference to be seen between the different autoencoders. This is supported by the reconstruction errors over the whole test set. The table below lists the summed binary cross-entropy averaged over the samples of the set.\nshallow vanilla stacked sparse denoising 123.2 63.92 87.87 64.27 75.15 vae beta_vae_strict beta_vae_loose vq 76.12 86.12 69.51 64.49 Trailing far behind, as suspected, is the shallow autoencoder. It simply lacks the capacity to capture the structure of MNIST. The vanilla autoencoder fares comparatively well, securing 1st place together with the sparse autoencoder and the vq-VAE. Both of the latter do not seem to suffer from their latent space restrictions in this regard. The VAE and beta-VAEs, on the other hand, do achieve a higher error, due to their restrictions. Besides, the sampling process in the VAEs introduces noise that harms the reconstruction error.\nWe will see later if reconstruction ability is a good proxy of performance on the other tasks.\nSampling The next trial will be a bit shorter, as only four contestants can sample unconditionally from their latent space: VAE, loose and strict beta-VAE, as well as vq-VAE. For each of them, we sample 16 latent codes and decode them. For the VAE and beta-VAEs, we sample from a standard Gaussian. The latent codes for the vq-VAE will be sampled uniformly from their learned categories.\nvae beta_vae_strict beta_vae_loose vq None of them look pretty, but we can glance at some differences, nonetheless. The most meaningful samples were generated by the strict beta-VAE. This was expected because its training laid the highest emphasis on enforcing the Gaussian prior. Its encoder output is therefore most similar to samples from a standard Gaussian which enables the decoder to successfully decode latent codes sampled from a true standard Gaussian.\nThe variance of images on the other hand is relatively small. We can see many fiveish, sixish-looking digits on the right, for example. The loose variant offers more diverse generated images, even though, less of them are legible. This makes sense, as the loose beta-VAE laid less emphasis on the prior and could encode more information into the latent space. Then again, it’s encoder outputs are less similar to standard Gaussian samples, which is why it fails to assign the unconditioned samples any meaning most of the time. The standard VAE lays somewhere between the other two, which is no surprise here.\nA bit of a disappointment is the sampled images from the vq-VAE. They do not resemble MNIST digits at all. The vq-VAE does not seem to support easy sampling by uniformly drawing categories and a more complex, learned prior seems, indeed, necessary.\nInterpolation The interpolation task shows us how densely the regions of the latent space are populated. We encode the first two images from the test set, a two and a seven, and interpolate linearly between them. The interpolations are then decoded to receive a new image. If the images from the interpolated latent codes show meaningful digits, the latent space between the class regions was effectively used by the autoencoder.\nFor all VAE types, we interpolate before the bottleneck operation. This means that we interpolate the Gaussian parameters and then sample from them for the VAE and beta-VAE. For the vq-VAE, we first interpolate and then quantize.\nshallow vanilla stacked sparse denoising vae beta_strict beta_loose vq Above you can see GIFs looping through the interpolated images back and forth, briefly stopping at the original images. We can see that the VAE and beta-VAEs produce relatively meaningful interpolations, going from a two over something looking threeish to a seven. The jitter you see is an artifact of the sampling process in the bottleneck. It could be avoided through the reparametrization trick, where we could sample only once from the standard Gaussian for the whole interpolation process.\nThe rest of the autoencoders do not seem to produce meaningful interpolations at all. They simply fade out pixels not needed and fade in new ones, while the VAE seems to bend the digit from one class to the other. Even though the vq-VAE is a VAE in theory and name, too, it does not show this property. Why the interpolation of the vq-VAE looks more like the vanilla one than the other VAE ones, becomes apparent when looking at the latent space.\nLatent Space Structure We are talking about the latent space a lot in this article, but how does it look like? Unfortunately, conceptualizing a 20-dimensional space in our mind is not a task humans were designed for at all, but there are tools at our disposal that let us overcome this hurdle. The UMAP algorithm is the go-to tool when it comes to visualizing high-dimensional spaces. It can reduce the latent space to two dimensions while preserving the neighborhood of the latent codes. Why look at the latent space of an autoencoder with two dimensions when you can have a pretty accurate representation of a 20-dimensional one? In the picture below, you can see a scatter plot for each of our autoencoders’ latent spaces. Each point is the latent code for one image from the test set and the color represents the digit in the image. The scales of the x- and y-axis don’t have any specific meaning.\nThere are some obvious similarities between the plots. First of all, we can see that the plots of vanilla, stacked, sparse, denoising, and vq-VAE are quite similar if we ignore different rotations. The clusters of zeros (blue), ones (orange), twos (green), and sixes (pink) are always nicely separated, so they seem to be quite different from the other digits. Then, we have a cluster of 4-7-9 (purple-gray-turquoise) and a cluster of 3-5-8 (red-brown-yellow). This hints at a connection between these digits, e.g swap the upper vertical line of a three and you get a five, add two more vertical lines and you get an eight. The autoencoders seem to encode the structural similarities between the digits.\nThe shallow autoencoder seems to struggle to separate the digits into clusters. While the cluster of ones is nicely separated, the other digits’ clusters are overlapping. This means that the shallow autoencoder maps some instances of different digits to the same latent code, leading to weird results we have seen in the reconstructions.\nThe plots of the VAE and beta-VAEs seem quite different from the others. Here we can see the influence of the divergence loss really well. The latent codes occupy less space than the other autoencoders’ codes because the divergence loss restricts them to be samples of a standard Gaussian. The strict beta-VAE uses the least space because the divergence loss is enforced the most.\nWe can explain the interpolation behavior of the VAEs, too. The VAE and strict beta-VAE produce something looking like a three during the interpolation, while the loose beta-VAE does not. For both of the former, the clusters of two (green) and three (red) are neighbors and are completely separated for the latter. It seems plausible that we may pass the border of the cluster of threes during interpolation.\nAs mentioned before, the latent space of the vq-VAE looks much more like the vanilla one’s than the latent space of the other VAEs. This seems to be a plausible explanation for why the interpolations of the vq-VAE look like the ones of the vanilla autoencoder.\nClassification Ok, so looking at generated images and latent spaces is nice and all, but what if we want to do something productive with an autoencoder? The latent space plots have shown us that some autoencoders are quite good at separating the digit classes in MNIST, although they did not receive any labels. Let’s take a look at how we can leverage this finding for classifying the MNIST digits.\nWe will use the 20-dimensional latent codes from our trained autoencoders and fit a dense classification layer on it. The layer will be trained on only 550 labeled samples of the training set against cross-entropy. In other words, we are using our autoencoders for semi-supervised learning. For comparison, training the vanilla encoder plus classification layer from scratch yields an accuracy of $0.4364$. We will see if our autoencoders can improve on that in the table below.\nshallow vanilla stacked sparse denoising 0.4511 0.7835 0.6107 0.2221 0.8049 vae beta_vae_strict beta_vae_loose vq 0.6016 0.4536 0.6311 0.7717 Unsurprisingly, nearly all autoencoders improved on the baseline, with the sparse one being the exception. The denoising autoencoder takes the first place, closely followed by the vanilla autoencoder and vq-VAE. It seems that the added input noise of the denoising autoencoder produces features that generalize best for classification.\nThe most interesting is in my opinion, that even the shallow autoencoder slightly improves the accuracy, even though it has only one layer and much fewer parameters. This shows once again that intelligent data usage often beats bigger models.\nThe VAE and beta-VAE show again how the divergence loss restricts the latent space. The loose beta-VAE achieves the highest accuracy as it can encode much more information than the other two. An interesting use case would be sampling multiple latent codes and classifying them. This could give a better measure of classification uncertainty. A quick Google search did not yield any papers on this, but it would be weird if nobody tried this already.\nThe results of the sparse autoencoder warrant a closer inspection, as they are much worse than the baseline. It seems that sparse features are not suited for classifying MNIST at all. We would need to test the sparse autoencoder on other datasets to see if it works better.\nAnomaly Detection In anomaly detection, or novelty detection to be specific, we want to find outlier samples in our test data, given our training data has no such outliers. Common applications are network intrusion detection or fault detection in predictive maintenance. We will fabricate an anomaly detection task from MNIST by excluding all images of ones from the training data. Afterward, we will see if the trained model can separate the ones in the test set from the other digits.\nDoing anomaly detection with autoencoders is relatively straight-forward. We take the trained model and calculate the reconstruction loss for our test samples. This is our anomaly score. If the reconstruction of a sample is good, it is probably similar to the training data. If the reconstruction is bad, the sample is considered an outlier. The autoencoder is expected to exploit correlations between features of the training data to learn an effective, low-dimensional representation. As long as these correlations are present, a test sample can be reconstructed quite well. If any of the correlations are not holding for a test sample, the autoencoder will fail to reconstruct it, to a degree.\nAn obvious challenge for this task is to find the optimal threshold for the anomaly score to consider a sample an anomaly. We will leave this problem for someone else to solve (as most publications do, too) and report the ROC plot and the area under the curve (AUC).\nThe results of this last task came as a bit of a surprise for me. I did some digging for bugs, but I am sure now that they are valid. The shallow autoencoder leaves all other autoencoders in the dust with a whooping $0.91$ AUC. Up next would be the stacked autoencoder with a narrow field of the remaining ones behind it. This order is stable for different digits and dimensions of the latent space, too.\nSo, why is that? How are the two autoencoders that trailed behind in almost all other tasks so good at anomaly detection? It seems that all other tasks rely on how well the learned latent space is able to generalize. The better it is at that, the better you can classify on it, the better you can reconstruct unseen samples. This time we do not want to generalize. At least not too well. We rely on the fact that samples too dissimilar to the training data cannot be reconstructed. If our latent space generalizes too well, we are hurting our anomaly detection performance.\nThe ability of an autoencoder to generalize is dependent on its encoder and decoder capacity, as well as the dimensionality of the latent space. We already lowered the dimensionality of the latent space to two for the anomaly detection, so there is not much room to go lower. Through its limited capacity, the shallow autoencoder struck the right balance of how well it modeled the training data. The stacked autoencoder’s greedy layer-wise training is limiting its ability to generalize similarly.\nIn the end, we may have to conclude that MNIST is just too damn easy. The deep autoencoders have much more capacity than it is helpful for this dataset. Of course, there are other perspectives, too. We could argue that images of ones are well within the domain of the training data, as it is a digit, too. The choice of our anomaly class was completely arbitrary. Maybe another autoencoder would have been most successful in separating digits from letters, but all this is a topic for another time.\nConclusion What a ride! We had our hypotheses confirmed, our latent spaces visualized and, finally, our expectations subverted. Do we have a conclusive champion of autoencoders? No, we do not. In the end, it is, again, a matter of the right tool for the job. You want to generate faces from noise? Use a variational autoencoder. They are too blurry? Try the vq-VAE with a learned prior. Classification? The denoising autoencoder seems fine. If you are doing anomaly detection, maybe try a shallow one first or even PCA. Sometimes less is better.\nAs I said in the beginning, this article cannot draw any conclusions beyond MNIST, but it serves as a good pointer, nevertheless. It would be interesting to do another run with a more complex dataset or one where the sparse autoencoder had its time to shine. A more focused look at what goes on in anomaly detection would be a nice thing, too.\nAs always, more questions generated than answered.\n","wordCount":"4844","inLanguage":"en","datePublished":"2021-01-24T00:00:00Z","dateModified":"2021-01-24T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tilman151.github.io/posts/ae-bakeoff/"},"publisher":{"@type":"Organization","name":"Don't Repeat Yourself","logo":{"@type":"ImageObject","url":"https://tilman151.github.io/img/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://tilman151.github.io/ accesskey=h title="Don't Repeat Yourself (Alt + H)">Don't Repeat Yourself</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://tilman151.github.io/publications/ title=Publications><span>Publications</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">The Great Autoencoder Bake Off</h1><div class=post-meta><span title='2021-01-24 00:00:00 +0000 UTC'>24 January 2021</span>&nbsp;·&nbsp;23 min</div></header><div class=post-content><p><em>&ldquo;Another article comparing types of autoencoders?&rdquo;</em>, you may think.
<em>&ldquo;There are already so many of them!&rdquo;</em>, you may think.
<em>&ldquo;How does he know what I am thinking?!&rdquo;</em>, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.</p><p>There are indeed articles comparing some autoencoders to each other (e.g. <a href=https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64>[1]</a>, <a href=https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924>[2]</a>, <a href=https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73>[3]</a>), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this <a href=https://github.com/AntixK/PyTorch-VAE>repository</a> I found, I took it into my own hands, and thus this blog post came into existence.</p><p>Of course, this article will probably not be a complete list of all the autoencoders out there, there are just too many.
But, if you are missing your favorite autoencoder, feel free to contact me or send a PR.
I tried to structure the code to be easily extendable, and you can find it at <a href=https://github.com/tilman151/ae_bakeoff>github.com/tilman151/ae_bakeoff</a>.</p><p>This is my first personal project with <em>pytorch-lightning</em>, too.
It really helped to make my code more structured, readable and cut out tons of boilerplate code.
You can check out this great package <a href=https://github.com/PyTorchLightning/pytorch-lightning>here</a>.</p><h2 id=what-and-how-are-we-comparing>What and how are we comparing?<a hidden class=anchor aria-hidden=true href=#what-and-how-are-we-comparing>#</a></h2><p>First, let&rsquo;s make clear what the inclusion criterion for this article is.
We are interested in all modifications to a vanilla deep autoencoder that change the way the latent space behaves or may improve a downstream task.
This excludes application-specific losses (e.g. perceptual VGG19 loss for images) and the types of encoder and decoder (e.g. LSTM vs. CNN).
The following types have made the cut:</p><ul><li>Shallow Autoencoders</li><li>Deep Autoencoders (vanilla AE)</li><li>Stacked Autoencoders</li><li>Sparse Autoencoders</li><li>Denoising Autoencoders</li><li>Variational Autoencoders (VAE)</li><li>Beta Variational Autoencoders (beta-VAE)</li><li>Vector-Quantized Variational Autoencoders (vq-VAE)</li></ul><p>Aside from describing what makes each of them unique, we will compare them in the following categories:</p><ul><li><strong>(Update 03.03.2021)</strong> Training time</li><li>Reconstruction quality</li><li>Quality of decoded samples from the latent space (if possible)</li><li>Quality of latent space interpolation</li><li>Structure of the latent space visualized with <a href=https://github.com/lmcinnes/umap>UMAP</a></li><li>ROC curve for anomaly detection with the reconstruction error</li><li>Classification accuracy of a linear layer fitted on the autoencoder&rsquo;s features</li></ul><p>All autoencoders tested share the same simple architecture with a fully-connected en- and decoder, batch normalization, and ReLU activation functions.
The output layer features a sigmoid activation function.
Save for the shallow one, all autoencoders have three encoder and decoder layers each.</p><p>The dimension of the latent space and the number of network parameters is held approximately constant.
This means that a variational autoencoder will have more parameters than a vanilla one as the encoder has to produce $2n$ outputs for a latent space of dimension $n$.
We make two training runs for each autoencoder: once with a latent space of 20 dimensions, and once with 2 dimensions.
The model from the second training run is used for anomaly detection, the first one for all other tasks.
Both variants are undercomplete, meaning that they have a higher input dimension than latent space dimension.</p><p>The MNIST dataset will serve as the venue for our contest.
The default train/test split is further divided into a train/val/test split by taking a random sample of 5000 samples from the training split for validation.</p><p>Obviously, there are some limitations to what we are doing here.
All training runs are only done once, so we have no measure of how stable the performances are.
We are only using one dataset, so drawing any generalized conclusions is off-limits.
Nevertheless, this is a blog and not a journal, so I think we are fine.
With all that in mind, let&rsquo;s begin our <em>Great Autoencoder Bake Off</em>.</p><h2 id=the-contestants>The Contestants<a hidden class=anchor aria-hidden=true href=#the-contestants>#</a></h2><p>First, we will have a look at how each tested autoencoder works and what makes them special.
From that, we will try to form a few hypotheses on how they will perform on the tasks.</p><h3 id=shallow-autoencoder>Shallow Autoencoder<a hidden class=anchor aria-hidden=true href=#shallow-autoencoder>#</a></h3><p>The shallow autoencoder is not really a contestant as it is far too underpowered to keep up with the others.
It is only here as a baseline.</p><p>$$\hat{x} = \operatorname{dec}(\operatorname{enc}(x))$$</p><p>Above you can see the reconstruction formula of the shallow autoencoder.
The formula declares how an autoencoder reconstructs a sample $x$ in a semi-mathematical way.</p><p>A shallow autoencoder features only one layer in its encoder and decoder each.
To distinguish the shallow autoencoder from a PCA, it uses a ReLU activation function in the encoder and a sigmoid in the decoder.
It is therefore non-linear.</p><p>Normally, the default choice for the reconstruction loss for an autoencoder is mean squared error.
We will use binary cross-entropy (BCE) because it yielded better-looking images in the preliminary experiments.
If you want to know why this is a valid choice of a loss function, I recommend reading chapters 5.5 and 6.2.1 of the <a href=https://www.deeplearningbook.org/>Deep Learning Book</a>.
As all following autoencoders, the shallow one is trained against the following version of BCE:</p><p>$$\mathcal{L}_r = \frac{1}{N}\sum_{i=1}^N \sum_{j=1}^{h\cdot w} \operatorname{BCE}(\hat{x}_j^{(i)}, x_j^{(i)})$$</p><p>where $x^{(i)}_j$ is the $j$th pixel of the $i$th input image and $\hat{x}^{(i)}_j$ the corresponding reconstruction.
The loss is, hence, summed up per image, and averaged over the batch.
This decision will become important for the variational autoencoder.</p><h3 id=deep-autoencoder>Deep Autoencoder<a hidden class=anchor aria-hidden=true href=#deep-autoencoder>#</a></h3><p>The deep autoencoder (a.k.a. vanilla autoencoder) is the big brother of the shallow one.
They are basically the same, but the deep autoencoder has more layers.
Therefore, they have the same reconstruction formula, too.</p><p>The deep autoencoder has no restrictions on its latent space and should, as a consequence, be able to encode the most information in it.</p><h3 id=stacked-autoencoder>Stacked Autoencoder<a hidden class=anchor aria-hidden=true href=#stacked-autoencoder>#</a></h3><p>The stacked autoencoder is a &ldquo;hack&rdquo; to get a deep autoencoder by training only shallow ones.
Instead of training the autoencoder end-to-end, we train it in a layer-wise, greedy fashion.
First, we take the first encoder and last decoder layer to form a shallow autoencoder.
After training these layers, we encode the whole dataset with the encoding layer and form another shallow autoencoder from the second encoder and next-to-last decoder layer.
This second shallow autoencoder is trained with the encoded dataset.
This process is repeated until we arrive at the innermost layers.
In the end, we get a deep autoencoder out of <em>stacked</em> shallow autoencoders.</p><p>We will train each of the $n$ layers for $\frac{1}{n}th$ of the total training epochs.</p><p>As the stacked autoencoder differs from a deep one only by training procedure, it has the same reconstruction function, too.
Again, we have no restrictions on the latent space, but the encoding is expected to be worse due to the greedy training procedure.</p><h3 id=sparse-autoencoder>Sparse Autoencoder<a hidden class=anchor aria-hidden=true href=#sparse-autoencoder>#</a></h3><p>The sparse autoencoder imposes a sparsity constraint on the latent code.
Each element in the latent code should only be active with a probability $p$.
We add the following auxiliary loss to enforce it while training:</p><p>$$L_s(z) = \sum_{i=1}^N \left( p \cdot \log{\frac{p}{\bar{z}^{(i)}}} + (1 - p) \cdot \log{\frac{1 - p}{1 - \bar{z}^{(i)}}} \right)$$</p><p>where $\bar{z}^{(i)}$ is the average activation of the $i$th element of the latent code over a batch.
This loss corresponds to the sum of $|z|$ Kulback-Leibler divergences between binomial distributions with the means $p$ and $\bar{z}^{(i)}$.
Other implementations may be possible to enforce the sparsity constraint, too.</p><p>To make this sparsity loss possible, we have to scale the latent code to $[0, 1]$ in order to be interpreted as probabilities.
This is done with a sigmoid activation function which gives us following reconstruction formula:</p><p>$$\hat{x} = \operatorname{dec}(\sigma(\operatorname{enc}(x)))$$</p><p>The complete loss for the sparse autoencoder combines the reconstruction loss and sparsity loss with an influence hyperparameter $\beta$:</p><p>$$L = L_r + \beta L_s$$</p><p>We will set $p$ to 0.25 and $\beta$ to one for all experiments.</p><h3 id=denoising-autoencoder>Denoising Autoencoder<a hidden class=anchor aria-hidden=true href=#denoising-autoencoder>#</a></h3><p>The denoising autoencoder does not restrict the latent space but aims to learn a more efficient encoding through applying noise to the input data.
Instead of feeding the input data straight to the network, we add Gaussian noise as follows:</p><p>$$x&rsquo; = \operatorname{clip} (x + \mathcal{N}(0;\operatorname{diag}(\beta\mathbf{I}))) $$</p><p>where $\operatorname{clip}$ is clipping its input to $[0, 1]$ and the scalar $\beta$ is the variance of the noise.
Hence, the autoencoder is trained on reconstructing clean samples from a noisy version.
The reconstruction formula is:</p><p>$$\hat{x} = \operatorname{dec}(\operatorname{enc}(x&rsquo;))$$</p><p>We use the noisy input exclusively in training.
When evaluating the autoencoder, we feed it the original input data.
Otherwise, the denoising autoencoder uses the same loss function as the ones before.
For all experiments, $\beta$ is set to 0.5.</p><h3 id=variational-autoencoder>Variational Autoencoder<a hidden class=anchor aria-hidden=true href=#variational-autoencoder>#</a></h3><p>In theory, the variational autoencoder (VAE) has not that much to do with a vanilla one.
In practice, the implementation and training are very similar.
The VAE interprets the reconstruction as a stochastic process, making it non-deterministic.
The encoder does not output the latent code, but the parameters of a probability distribution of latent codes.
The decoder then receives a sample from this distribution.
The default choice of distribution family is a Gaussian $\mathcal{N}(\mu; \operatorname{diag}(\Sigma))$.
The reconstruction formula looks as follows:</p><p>$$\hat{x} = \operatorname{dec}(\operatorname{sample}(\operatorname{enc}_\mu(x), \operatorname{enc}_\Sigma(x)))$$</p><p>where $\operatorname{enc}_\mu(x)$ and $\operatorname{enc}_\Sigma(x)$ encode $x$ to $\mu$ and $\Sigma$.
Both encoders share most of their parameters.
In practice, a single encoder simply gets two output layers instead of one.
The problem is now that sampling from a distribution is an operation without gradient and backpropagation from the decoder to the encoder would be impossible.
The solution is called the reparametrization trick and transforms a sample from a standard Gaussian to one from a Gaussian parametrized by $\mu$ and $\Sigma$:</p><p>$$\operatorname{reparametrize}(\mu, \Sigma) = \Sigma \cdot \operatorname{sample}(0, \operatorname{diag}(\mathbf{I})) + \mu$$</p><p>This formulation has a gradient with respect to $\mu$ and $\Sigma$ and makes training with backpropagation possible.</p><p>The variational autoencoder further restricts the latent space by requiring Gaussian distributions to be similar to a standard Gaussian.
The distribution parameters are therefore penalized with the Kulback-Leibler divergence:</p><p>$$L_{KL} = \frac{1}{2N}\sum_{i=1}^N \sum_{j=1}^{|z|} (2\Sigma_j^{(i)} + (\mu_j^{(i)})^2 - 1 - 2\log{(\Sigma_j^{(i)})})$$</p><p>The divergence is averaged over the batch.
The reconstruction loss is, as mentioned before, averaged in the same way to retain the correct ratio between the reconstruction loss and the divergence loss.
The full training loss is then:</p><p>$$L = L_r + L_{KL}$$</p><p>As we try to constrain the encoder to output standard Gaussians, we can try to decode a sample from a standard Gaussian directly, too.
This unconditioned sampling is a unique property of VAEs and makes them generative models, similar to GANs.
The formula for unconditional sampling is:</p><p>$$\hat{x} = \operatorname{dec}(\operatorname{sample}(0, \operatorname{diag}(\mathbf{I}))$$</p><p>If you want to understand the theory behind VAEs, I recommend the <a href=https://arxiv.org/abs/1606.05908>tutorial</a> by Carl Doersch.</p><h3 id=beta-variational-autoencoder>Beta Variational Autoencoder<a hidden class=anchor aria-hidden=true href=#beta-variational-autoencoder>#</a></h3><p>The beta VAE is a generalization of the VAE that simply changes the ratio between reconstruction and divergence loss.
The influence of the divergence loss is parametrized by a scalar $\beta$, hence the name:</p><p>$$L = L_r + \beta L_{KL}$$</p><p>A $\beta &lt; 1$ relaxes the constraints on the latent space while a $\beta > 1$ makes the constraint stricter.
The former should result in better reconstructions and the latter in better unconditional sampling.
The theoretic derivation makes good arguments about what else this simple change affects.
You can read them on <a href="https://openreview.net/pdf?id=Sy2fzU9gl">OpenReview</a>.
We will use a strict version of this autoencoder with $\beta = 2$ and a loose version with $\beta = 0.5$.</p><h3 id=vector-quantized-variational-autoencoder>Vector-Quantized Variational Autoencoder<a hidden class=anchor aria-hidden=true href=#vector-quantized-variational-autoencoder>#</a></h3><p>The vector-quantized variational autoencoder (vq-VAE) is a VAE that uses a uniform categorical distribution to generate its latent codes.
Each element of the encoder output is replaced by the categorical value of the distribution that is its nearest neighbor.
This is a form of <em>quantization</em> and means that the latent space is not continuous anymore but discrete.</p><p>$$\hat{x} = \operatorname{dec}(\operatorname{quantize}(\operatorname{enc}(x))))$$</p><p>The categories themselves are learned by minimizing the sum squared error to the encoder outputs:</p><p>$$L_{vq} = \frac{1}{N}\sum_{i=1}^N \sum_{j=1}^{|z|} (\operatorname{sg}(z_j^{(i)}) - \hat{z}_j^{(i)})^2$$</p><p>where $z$ is the output of the encoder, $\hat{z}$ is the corresponding quantized latent code, and $sg$ the stop gradient operator.
On the other hand, the encoder is encouraged to output encodings similar to the categories by sum squared error, too:</p><p>$$L_{c} = \frac{1}{N}\sum_{i=1}^N \sum_{j=1}^{|z|} (z_j^{(i)} - \operatorname{sg}(\hat{z}_j^{(i)}))^2$$</p><p>This is called the commitment loss.
A Kulback-Leibler divergence loss is not necessary, as the divergence would be a constant for a uniform categorical distribution.
Both losses are combined with the reconstruction loss by a hyperparameter $\beta$ controlling the influence of the commitment loss:</p><p>$$L = L_r + L_{vq} + \beta L_c$$</p><p>We set $\beta$ to one for all experiments.</p><p>Because the vq-VAE is a generative model, we can sample from it unconditionally, too.
In the original paper, a pixel-CNN is used to autoregressively sample a latent code.
For simplicity&rsquo;s sake, we will sample by drawing categories uniformly.</p><h2 id=the-gauntlet>The Gauntlet<a hidden class=anchor aria-hidden=true href=#the-gauntlet>#</a></h2><p>Now that we know who we are dealing with, let&rsquo;s see how they fare in our gauntlet of tasks.
Besides the standard ones like reconstruction quality, we will have a look at semi-supervised classification and anomaly detection, too.</p><h3 id=training-time>Training Time<a hidden class=anchor aria-hidden=true href=#training-time>#</a></h3><p><strong>(Update 03.03.2021)</strong>
As per feedback from Reddit, we are going to look at the training times of the autoencoders.
The time was not recorded directly because we would need to rerun the experiments to do that.
Instead, we are using the TensorBoard event files and calculate the time between the first training loss and last validation loss logged.
This gives us only an offset of the iteration time of the first training batch.
The times should, therefore, be interpreted as the time to run 60 epochs with validation after each one.</p><table><thead><tr><th style=text-align:center>shallow</th><th style=text-align:center>vanilla</th><th style=text-align:center>stacked</th><th style=text-align:center>sparse</th><th style=text-align:center>denoising</th></tr></thead><tbody><tr><td style=text-align:center>0:10:38</td><td style=text-align:center>0:11:15</td><td style=text-align:center>0:11:05</td><td style=text-align:center>0:11:27</td><td style=text-align:center>0:11:18</td></tr></tbody></table><table><thead><tr><th style=text-align:center>vae</th><th style=text-align:center>beta_vae_strict</th><th style=text-align:center>beta_vae_loose</th><th style=text-align:center>vq</th></tr></thead><tbody><tr><td style=text-align:center>0:11:30</td><td style=text-align:center>0:11:29</td><td style=text-align:center>0:11:28</td><td style=text-align:center>0:11:27</td></tr></tbody></table><p>Unsurprisingly, there is not much of a difference.
Even the shallow autoencoder, that is much smaller, takes only about half a minute less time.
This indicates that the time for the forward and backward passes is much smaller than the overhead of data loading, logging, etc.
Some variation can be attributed to this being computed on a Windows machine, too.
Who knows what background process is hogging resources at any given time?</p><h3 id=reconstruction>Reconstruction<a hidden class=anchor aria-hidden=true href=#reconstruction>#</a></h3><p>First and foremost, we want to see how well each autoencoder can reconstruct its input.
For this, we can have a look at the 16 first images of the MNIST test set.</p><table><thead><tr><th style=text-align:center>shallow</th><th style=text-align:center>vanilla</th><th style=text-align:center>stacked</th><th style=text-align:center>sparse</th><th style=text-align:center>denoising</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/shallow.jpeg alt="Shallow AE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/vanilla.jpeg alt="Vanilla AE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/stacked.jpeg alt="Stacked AE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/sparse.jpeg alt="Sparse AE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/denoising.jpeg alt="Denoising AE Reconstructions" style=max-width:unset></td></tr></tbody></table><table><thead><tr><th style=text-align:center>vae</th><th style=text-align:center>beta_vae_strict</th><th style=text-align:center>beta_vae_loose</th><th style=text-align:center>vq</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/vae.jpeg alt="VAE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/beta_vae_strict.jpeg alt="Strict beta-VAE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/beta_vae_loose.jpeg alt="Loose beta-VAE Reconstructions" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reconstructions/vq.jpeg alt="vq-VAE Reconstructions" style=max-width:unset></td></tr></tbody></table><p>The shallow autoencoder fails to reconstruct many of the test samples correctly.
Fours and nines are barely distinguishable, and some digits are not recognizable at all.
The other autoencoders do a much better, but not perfect, job.
The denoising one is in the habit of making some thin lines disappear.
The sparse and stacked ones seem to have problems with abnormally formed digits like the five and one of the nines.
Overall the reconstructions are a little blurry, which is normal for autoencoders.
The vq-VAE produces slightly less blurry reconstructions, which the original authors credit to the discrete latent space.</p><p>Otherwise, there is little difference to be seen between the different autoencoders.
This is supported by the reconstruction errors over the whole test set.
The table below lists the summed binary cross-entropy averaged over the samples of the set.</p><table><thead><tr><th style=text-align:center>shallow</th><th style=text-align:center>vanilla</th><th style=text-align:center>stacked</th><th style=text-align:center>sparse</th><th style=text-align:center>denoising</th></tr></thead><tbody><tr><td style=text-align:center>123.2</td><td style=text-align:center>63.92</td><td style=text-align:center>87.87</td><td style=text-align:center>64.27</td><td style=text-align:center>75.15</td></tr></tbody></table><table><thead><tr><th style=text-align:center>vae</th><th style=text-align:center>beta_vae_strict</th><th style=text-align:center>beta_vae_loose</th><th style=text-align:center>vq</th></tr></thead><tbody><tr><td style=text-align:center>76.12</td><td style=text-align:center>86.12</td><td style=text-align:center>69.51</td><td style=text-align:center>64.49</td></tr></tbody></table><p>Trailing far behind, as suspected, is the shallow autoencoder.
It simply lacks the capacity to capture the structure of MNIST.
The vanilla autoencoder fares comparatively well, securing 1st place together with the sparse autoencoder and the vq-VAE.
Both of the latter do not seem to suffer from their latent space restrictions in this regard.
The VAE and beta-VAEs, on the other hand, do achieve a higher error, due to their restrictions.
Besides, the sampling process in the VAEs introduces noise that harms the reconstruction error.</p><p>We will see later if reconstruction ability is a good proxy of performance on the other tasks.</p><h3 id=sampling>Sampling<a hidden class=anchor aria-hidden=true href=#sampling>#</a></h3><p>The next trial will be a bit shorter, as only four contestants can sample unconditionally from their latent space: VAE, loose and strict beta-VAE, as well as vq-VAE.
For each of them, we sample 16 latent codes and decode them.
For the VAE and beta-VAEs, we sample from a standard Gaussian.
The latent codes for the vq-VAE will be sampled uniformly from their learned categories.</p><table><thead><tr><th style=text-align:center>vae</th><th style=text-align:center>beta_vae_strict</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/samples/vae.jpeg alt="VAE Samples" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/samples/beta_vae_strict.jpeg alt="Strict beta-VAE Samples" style=max-width:unset></td></tr></tbody></table><table><thead><tr><th style=text-align:center>beta_vae_loose</th><th style=text-align:center>vq</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/samples/beta_vae_loose.jpeg alt="Loose beta-VAE Samples" style=max-width:unset></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/samples/vq.jpeg alt="vq-VAE Samples" style=max-width:unset></td></tr></tbody></table><p>None of them look pretty, but we can glance at some differences, nonetheless.
The most meaningful samples were generated by the strict beta-VAE.
This was expected because its training laid the highest emphasis on enforcing the Gaussian prior.
Its encoder output is therefore most similar to samples from a standard Gaussian which enables the decoder to successfully decode latent codes sampled from a true standard Gaussian.</p><p>The variance of images on the other hand is relatively small.
We can see many fiveish, sixish-looking digits on the right, for example.
The loose variant offers more diverse generated images, even though, less of them are legible.
This makes sense, as the loose beta-VAE laid less emphasis on the prior and could encode more information into the latent space.
Then again, it&rsquo;s encoder outputs are less similar to standard Gaussian samples, which is why it fails to assign the unconditioned samples any meaning most of the time.
The standard VAE lays somewhere between the other two, which is no surprise here.</p><p>A bit of a disappointment is the sampled images from the vq-VAE.
They do not resemble MNIST digits at all.
The vq-VAE does not seem to support easy sampling by uniformly drawing categories and a more complex, learned prior seems, indeed, necessary.</p><h3 id=interpolation>Interpolation<a hidden class=anchor aria-hidden=true href=#interpolation>#</a></h3><p>The interpolation task shows us how densely the regions of the latent space are populated.
We encode the first two images from the test set, a two and a seven, and interpolate linearly between them.
The interpolations are then decoded to receive a new image.
If the images from the interpolated latent codes show meaningful digits, the latent space between the class regions was effectively used by the autoencoder.</p><p>For all VAE types, we interpolate before the bottleneck operation.
This means that we interpolate the Gaussian parameters and then sample from them for the VAE and beta-VAE.
For the vq-VAE, we first interpolate and then quantize.</p><table><thead><tr><th style=text-align:center>shallow</th><th style=text-align:center>vanilla</th><th style=text-align:center>stacked</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/shallow.gif alt="Shallow Interpolation" style=max-width:unset;width:64px></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/vanilla.gif alt="Vanilla Interpolation" style=max-width:unset;width:64px></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/stacked.gif alt="Stacked Interpolation" style=max-width:unset;width:64px></td></tr></tbody></table><table><thead><tr><th style=text-align:center>sparse</th><th style=text-align:center>denoising</th><th style=text-align:center>vae</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/sparse.gif alt="Sparse Interpolation" style=max-width:unset;width:64px></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/denoising.gif alt="Denoising Interpolation" style=max-width:unset;width:64px></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/vae.gif alt="VAE Interpolation" style=max-width:unset;width:64px></td></tr></tbody></table><table><thead><tr><th style=text-align:center>beta_strict</th><th style=text-align:center>beta_loose</th><th style=text-align:center>vq</th></tr></thead><tbody><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/beta_vae_strict.gif alt="Beta Strict Interpolation" style=max-width:unset;width:64px></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/beta_vae_loose.gif alt="Beta Loose Interpolation" style=max-width:unset;width:64px></td><td style=text-align:center><img src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/interpolation/vq.gif alt="VQ Interpolation" style=max-width:unset;width:64px></td></tr></tbody></table><p>Above you can see GIFs looping through the interpolated images back and forth, briefly stopping at the original images.
We can see that the VAE and beta-VAEs produce relatively meaningful interpolations, going from a two over something looking threeish to a seven.
The jitter you see is an artifact of the sampling process in the bottleneck.
It could be avoided through the reparametrization trick, where we could sample only once from the standard Gaussian for the whole interpolation process.</p><p>The rest of the autoencoders do not seem to produce meaningful interpolations at all.
They simply fade out pixels not needed and fade in new ones, while the VAE seems to bend the digit from one class to the other.
Even though the vq-VAE is a VAE in theory and name, too, it does not show this property.
Why the interpolation of the vq-VAE looks more like the vanilla one than the other VAE ones, becomes apparent when looking at the latent space.</p><h3 id=latent-space-structure>Latent Space Structure<a hidden class=anchor aria-hidden=true href=#latent-space-structure>#</a></h3><p>We are talking about the latent space a lot in this article, but how does it look like?
Unfortunately, conceptualizing a 20-dimensional space in our mind is not a task humans were designed for at all, but there are tools at our disposal that let us overcome this hurdle.
The UMAP algorithm is the go-to tool when it comes to visualizing high-dimensional spaces.
It can reduce the latent space to two dimensions while preserving the neighborhood of the latent codes.
Why look at the latent space of an autoencoder with two dimensions when you can have a pretty accurate representation of a 20-dimensional one?
In the picture below, you can see a scatter plot for each of our autoencoders&rsquo; latent spaces.
Each point is the latent code for one image from the test set and the color represents the digit in the image.
The scales of the x- and y-axis don&rsquo;t have any specific meaning.</p><p><img alt="Latent Space Visualization in 2D" loading=lazy src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/reduction.png></p><p>There are some obvious similarities between the plots.
First of all, we can see that the plots of vanilla, stacked, sparse, denoising, and vq-VAE are quite similar if we ignore different rotations.
The clusters of zeros (blue), ones (orange), twos (green), and sixes (pink) are always nicely separated, so they seem to be quite different from the other digits.
Then, we have a cluster of 4-7-9 (purple-gray-turquoise) and a cluster of 3-5-8 (red-brown-yellow).
This hints at a connection between these digits, e.g swap the upper vertical line of a three and you get a five, add two more vertical lines and you get an eight.
The autoencoders seem to encode the structural similarities between the digits.</p><p>The shallow autoencoder seems to struggle to separate the digits into clusters.
While the cluster of ones is nicely separated, the other digits&rsquo; clusters are overlapping.
This means that the shallow autoencoder maps some instances of different digits to the same latent code, leading to weird results we have seen in the reconstructions.</p><p>The plots of the VAE and beta-VAEs seem quite different from the others.
Here we can see the influence of the divergence loss really well.
The latent codes occupy less space than the other autoencoders&rsquo; codes because the divergence loss restricts them to be samples of a standard Gaussian.
The strict beta-VAE uses the least space because the divergence loss is enforced the most.</p><p>We can explain the interpolation behavior of the VAEs, too.
The VAE and strict beta-VAE produce something looking like a three during the interpolation, while the loose beta-VAE does not.
For both of the former, the clusters of two (green) and three (red) are neighbors and are completely separated for the latter.
It seems plausible that we may pass the border of the cluster of threes during interpolation.</p><p>As mentioned before, the latent space of the vq-VAE looks much more like the vanilla one&rsquo;s than the latent space of the other VAEs.
This seems to be a plausible explanation for why the interpolations of the vq-VAE look like the ones of the vanilla autoencoder.</p><h3 id=classification>Classification<a hidden class=anchor aria-hidden=true href=#classification>#</a></h3><p>Ok, so looking at generated images and latent spaces is nice and all, but what if we want to do something productive with an autoencoder?
The latent space plots have shown us that some autoencoders are quite good at separating the digit classes in MNIST, although they did not receive any labels.
Let&rsquo;s take a look at how we can leverage this finding for classifying the MNIST digits.</p><p>We will use the 20-dimensional latent codes from our trained autoencoders and fit a dense classification layer on it.
The layer will be trained on only 550 labeled samples of the training set against cross-entropy.
In other words, we are using our autoencoders for semi-supervised learning.
For comparison, training the vanilla encoder plus classification layer from scratch yields an accuracy of $0.4364$.
We will see if our autoencoders can improve on that in the table below.</p><table><thead><tr><th style=text-align:center>shallow</th><th style=text-align:center>vanilla</th><th style=text-align:center>stacked</th><th style=text-align:center>sparse</th><th style=text-align:center>denoising</th></tr></thead><tbody><tr><td style=text-align:center>0.4511</td><td style=text-align:center>0.7835</td><td style=text-align:center>0.6107</td><td style=text-align:center>0.2221</td><td style=text-align:center>0.8049</td></tr></tbody></table><table><thead><tr><th style=text-align:center>vae</th><th style=text-align:center>beta_vae_strict</th><th style=text-align:center>beta_vae_loose</th><th style=text-align:center>vq</th></tr></thead><tbody><tr><td style=text-align:center>0.6016</td><td style=text-align:center>0.4536</td><td style=text-align:center>0.6311</td><td style=text-align:center>0.7717</td></tr></tbody></table><p>Unsurprisingly, nearly all autoencoders improved on the baseline, with the sparse one being the exception.
The denoising autoencoder takes the first place, closely followed by the vanilla autoencoder and vq-VAE.
It seems that the added input noise of the denoising autoencoder produces features that generalize best for classification.</p><p>The most interesting is in my opinion, that even the shallow autoencoder slightly improves the accuracy, even though it has only one layer and much fewer parameters.
This shows once again that intelligent data usage often beats bigger models.</p><p>The VAE and beta-VAE show again how the divergence loss restricts the latent space.
The loose beta-VAE achieves the highest accuracy as it can encode much more information than the other two.
An interesting use case would be sampling multiple latent codes and classifying them.
This could give a better measure of classification uncertainty.
A quick Google search did not yield any papers on this, but it would be weird if nobody tried this already.</p><p>The results of the sparse autoencoder warrant a closer inspection, as they are much worse than the baseline.
It seems that sparse features are not suited for classifying MNIST at all.
We would need to test the sparse autoencoder on other datasets to see if it works better.</p><h3 id=anomaly-detection>Anomaly Detection<a hidden class=anchor aria-hidden=true href=#anomaly-detection>#</a></h3><p>In anomaly detection, or novelty detection to be specific, we want to find outlier samples in our test data, given our training data has no such outliers.
Common applications are network intrusion detection or fault detection in predictive maintenance.
We will fabricate an anomaly detection task from MNIST by excluding all images of ones from the training data.
Afterward, we will see if the trained model can separate the ones in the test set from the other digits.</p><p>Doing anomaly detection with autoencoders is relatively straight-forward.
We take the trained model and calculate the reconstruction loss for our test samples.
This is our anomaly score.
If the reconstruction of a sample is good, it is probably similar to the training data.
If the reconstruction is bad, the sample is considered an outlier.
The autoencoder is expected to exploit correlations between features of the training data to learn an effective, low-dimensional representation.
As long as these correlations are present, a test sample can be reconstructed quite well.
If any of the correlations are not holding for a test sample, the autoencoder will fail to reconstruct it, to a degree.</p><p>An obvious challenge for this task is to find the optimal threshold for the anomaly score to consider a sample an anomaly.
We will leave this problem for someone else to solve (as most publications do, too) and report the ROC plot and the area under the curve (AUC).</p><p><img alt="Anomaly Results with ROC plots and AUC" loading=lazy src=https://raw.githubusercontent.com/tilman151/ae_bakeoff/master/results/mnist/anomaly.png></p><p>The results of this last task came as a bit of a surprise for me.
I did some digging for bugs, but I am sure now that they are valid.
The shallow autoencoder leaves all other autoencoders in the dust with a whooping $0.91$ AUC.
Up next would be the stacked autoencoder with a narrow field of the remaining ones behind it.
This order is stable for different digits and dimensions of the latent space, too.</p><p>So, why is that?
How are the two autoencoders that trailed behind in almost all other tasks so good at anomaly detection?
It seems that all other tasks rely on how well the learned latent space is able to generalize.
The better it is at that, the better you can classify on it, the better you can reconstruct unseen samples.
This time we do not want to generalize.
At least not too well.
We rely on the fact that samples too dissimilar to the training data cannot be reconstructed.
If our latent space generalizes too well, we are hurting our anomaly detection performance.</p><p>The ability of an autoencoder to generalize is dependent on its encoder and decoder capacity, as well as the dimensionality of the latent space.
We already lowered the dimensionality of the latent space to two for the anomaly detection, so there is not much room to go lower.
Through its limited capacity, the shallow autoencoder struck the right balance of how well it modeled the training data.
The stacked autoencoder&rsquo;s greedy layer-wise training is limiting its ability to generalize similarly.</p><p>In the end, we may have to conclude that MNIST is just too damn easy.
The deep autoencoders have much more capacity than it is helpful for this dataset.
Of course, there are other perspectives, too.
We could argue that images of ones are well within the domain of the training data, as it is a digit, too.
The choice of our anomaly class was completely arbitrary.
Maybe another autoencoder would have been most successful in separating digits from letters, but all this is a topic for another time.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>What a ride!
We had our hypotheses confirmed, our latent spaces visualized and, finally, our expectations subverted.
Do we have a conclusive champion of autoencoders?
No, we do not.
In the end, it is, again, a matter of the right tool for the job.
You want to generate faces from noise?
Use a variational autoencoder.
They are too blurry?
Try the vq-VAE with a learned prior.
Classification?
The denoising autoencoder seems fine.
If you are doing anomaly detection, maybe try a shallow one first or even PCA.
Sometimes less is better.</p><p>As I said in the beginning, this article cannot draw any conclusions beyond MNIST, but it serves as a good pointer, nevertheless.
It would be interesting to do another run with a more complex dataset or one where the sparse autoencoder had its time to shine.
A more focused look at what goes on in anomaly detection would be a nice thing, too.</p><p>As always, more questions generated than answered.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://tilman151.github.io/tags/deeplearning/>Deeplearning</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://tilman151.github.io/>Don't Repeat Yourself</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><style>#cookie-notice{padding:1.5rem 1rem;display:none;text-align:center;position:fixed;bottom:0;width:100%;background:var(--border);color:var(--primary)}#cookie-notice a{display:inline-block;cursor:pointer;margin-left:1rem}@media(max-width:767px){#cookie-notice span{display:block;padding-top:3px;margin-bottom:1rem}#cookie-notice a{position:relative;bottom:4px}}</style><div id=cookie-notice><span>I am using third-party cookies to count readers. If you don't want to be counted, please use incognito mode.</span><a id=cookie-notice-accept class="btn btn-primary btn-sm">OK</a><a href=/privacy class="btn btn-primary btn-sm">More info</a></div><script>function createCookie(e,t,n){var s,o="";n&&(s=new Date,s.setTime(s.getTime()+n*24*60*60*1e3),o="; expires="+s.toUTCString()),document.cookie=e+"="+t+o+"; path=/"}function readCookie(e){for(var t,s=e+"=",o=document.cookie.split(";"),n=0;n<o.length;n++){for(t=o[n];t.charAt(0)==" ";)t=t.substring(1,t.length);if(t.indexOf(s)==0)return t.substring(s.length,t.length)}return null}function eraseCookie(e){createCookie(e,"",-1)}if(readCookie("cookie-notice-dismissed")=="true"){var s,clicky_site_ids=clicky_site_ids||[];clicky_site_ids.push(101271854),s=document.createElement("script"),s.src="https://static.getclicky.com/js",document.head.appendChild(s)}else document.getElementById("cookie-notice").style.display="block";document.getElementById("cookie-notice-accept").addEventListener("click",function(){createCookie("cookie-notice-dismissed","true",31),document.getElementById("cookie-notice").style.display="none",location.reload()})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>