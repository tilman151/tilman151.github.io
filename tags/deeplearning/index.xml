<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Deeplearning on Don't Repeat Yourself</title><link>https://krokotsch.eu/tags/deeplearning/</link><description>Recent content in Deeplearning on Don't Repeat Yourself</description><generator>Hugo -- 0.152.2</generator><language>en-us</language><lastBuildDate>Sun, 24 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://krokotsch.eu/tags/deeplearning/index.xml" rel="self" type="application/rss+xml"/><item><title>The Great Autoencoder Bake Off</title><link>https://krokotsch.eu/posts/ae-bakeoff/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://krokotsch.eu/posts/ae-bakeoff/</guid><description>&lt;p&gt;&lt;em&gt;&amp;ldquo;Another article comparing types of autoencoders?&amp;rdquo;&lt;/em&gt;, you may think.
&lt;em&gt;&amp;ldquo;There are already so many of them!&amp;rdquo;&lt;/em&gt;, you may think.
&lt;em&gt;&amp;ldquo;How does he know what I am thinking?!&amp;rdquo;&lt;/em&gt;, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.&lt;/p&gt;
&lt;p&gt;There are indeed articles comparing some autoencoders to each other (e.g. &lt;a href="https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64"&gt;[1]&lt;/a&gt;, &lt;a href="https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924"&gt;[2]&lt;/a&gt;, &lt;a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73"&gt;[3]&lt;/a&gt;), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this &lt;a href="https://github.com/AntixK/PyTorch-VAE"&gt;repository&lt;/a&gt; I found, I took it into my own hands, and thus this blog post came into existence.&lt;/p&gt;</description></item></channel></rss>