<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Deeplearning on Don't Repeat Yourself</title><link>https://tilman151.github.io/tags/deeplearning/</link><description>Recent content in Deeplearning on Don't Repeat Yourself</description><generator>Hugo -- 0.136.5</generator><language>en-us</language><lastBuildDate>Sun, 24 Jan 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://tilman151.github.io/tags/deeplearning/index.xml" rel="self" type="application/rss+xml"/><item><title>The Great Autoencoder Bake Off</title><link>https://tilman151.github.io/posts/ae-bakeoff/</link><pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate><guid>https://tilman151.github.io/posts/ae-bakeoff/</guid><description>&lt;p>&lt;em>&amp;ldquo;Another article comparing types of autoencoders?&amp;rdquo;&lt;/em>, you may think.
&lt;em>&amp;ldquo;There are already so many of them!&amp;rdquo;&lt;/em>, you may think.
&lt;em>&amp;ldquo;How does he know what I am thinking?!&amp;rdquo;&lt;/em>, you may think.
While the first two statements are certainly appropriate reactions - and the third a bit paranoid - let me explain my reasons for this article.&lt;/p>
&lt;p>There are indeed articles comparing some autoencoders to each other (e.g. &lt;a href="https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64">[1]&lt;/a>, &lt;a href="https://towardsdatascience.com/a-high-level-guide-to-autoencoders-b103ccd45924">[2]&lt;/a>, &lt;a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">[3]&lt;/a>), I found them lacking something.
Most only compare a hand full of types and/or only scratch the surface of what autoencoders can do.
Often you see only reconstructed samples, generated samples, or latent space visualization but nothing about downstream tasks.
I wanted to know if a stacked autoencoder is better than a sparse one for anomaly detection or if a variational autoencoder learns better features for classification than a vector-quantized one.
Inspired by this &lt;a href="https://github.com/AntixK/PyTorch-VAE">repository&lt;/a> I found, I took it into my own hands, and thus this blog post came into existence.&lt;/p></description></item></channel></rss>